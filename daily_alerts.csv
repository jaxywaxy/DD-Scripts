Title,Text,Date,Tags,Alert Type
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on the GET /naturalGas/getBillPeriodUsage resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  [![Metric Graph]()](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  %%%,2025-09-24 12:20:46,"env:prod,resource_hash:c707540c318d5861,resource_name:get_/naturalgas/getbillperiodusage,service:mobile-api-service,source:watchdog,story_category:apm,story_key:2ff3f01a-87d5-52d3-90a0-3c6f41e7adba,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on the GET /naturalGas/getBillPeriodUsage resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  [![Metric Graph]()](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:c707540c318d5861,resource_name:get_/naturalgas/getbillperiodusage,service:mobile-api-service,source:watchdog,story_category:apm,story_key:2ff3f01a-87d5-52d3-90a0-3c6f41e7adba,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:10:47,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on the GET /naturalGas/getBillPeriodUsage resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  [![Metric Graph]()](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  %%%,2025-09-24 12:00:40,"env:prod,resource_hash:c707540c318d5861,resource_name:get_/naturalgas/getbillperiodusage,service:mobile-api-service,source:watchdog,story_category:apm,story_key:2ff3f01a-87d5-52d3-90a0-3c6f41e7adba,story_type:error_rate,team:ge-integration",info
Error rate increased on the GET /naturalGas/getBillPeriodUsage resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  [![Metric Graph]()](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:c707540c318d5861,resource_name:get_/naturalgas/getbillperiodusage,service:mobile-api-service,source:watchdog,story_category:apm,story_key:2ff3f01a-87d5-52d3-90a0-3c6f41e7adba,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:50:09,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on the GET /naturalGas/getBillPeriodUsage resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  [![Metric Graph]()](/watchdog/story/2ff3f01a-87d5-52d3-90a0-3c6f41e7adba)  %%%,2025-09-24 11:48:48,"env:prod,resource_hash:c707540c318d5861,resource_name:get_/naturalgas/getbillperiodusage,service:mobile-api-service,source:watchdog,story_category:apm,story_key:2ff3f01a-87d5-52d3-90a0-3c6f41e7adba,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:39:55,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:30:42,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
[P5] [Recovered] CPU is at 100% for Mule Host,"%%%   **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/3865a975315d8e19a1b016f723f448692b025d78.png)](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758712532000&to_ts=1758713732000&event_id=8295504962352308205&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:art-api-p01,service:mule** was **> 0.0** at least once during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 11:25:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758712532000&to_ts=1758713732000&event_id=8295504962352308205&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View art-api-p01](/infrastructure?filter=art-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758713132000&to_ts=1758713552000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aart-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aart-api-p01&from_ts=1758713132000&to_ts=1758713432000&live=false&link_source=monitor_notif)] %%%",2025-09-24 11:30:32,"env:prod,host:art-api-p01,location:art_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",success
[P5] [Triggered] CPU is at 100% for Mule Host,"%%% Mule Host art-api-p01 is reporting 100% CPU  **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/22300a522276b24dbae047c0bcb5f940d03614e0.png)](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758712232000&to_ts=1758713432000&event_id=8295499926468556854&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:art-api-p01,service:mule** was **<= 0.0** at all times during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 11:25:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758712232000&to_ts=1758713432000&event_id=8295499926468556854&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View art-api-p01](/infrastructure?filter=art-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758712832000&to_ts=1758713252000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aart-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aart-api-p01&from_ts=1758712832000&to_ts=1758713132000&live=false&link_source=monitor_notif)] %%%",2025-09-24 11:25:32,"env:prod,host:art-api-p01,location:art_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",error
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:20:13,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
[P5] [Recovered] CPU is at 100% for Mule Host,"%%%   **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/4fec60f76931aee422af6f2fb4cab635416e5606.png)](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758711812000&to_ts=1758713012000&event_id=8295492881045433086&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:art-api-p01,service:mule** was **> 0.0** at least once during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 11:14:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758711812000&to_ts=1758713012000&event_id=8295492881045433086&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View art-api-p01](/infrastructure?filter=art-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758712412000&to_ts=1758712832000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aart-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aart-api-p01&from_ts=1758712412000&to_ts=1758712712000&live=false&link_source=monitor_notif)] %%%",2025-09-24 11:18:32,"env:prod,host:art-api-p01,location:art_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",success
[P5] [Triggered] CPU is at 100% for Mule Host,"%%% Mule Host art-api-p01 is reporting 100% CPU  **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/4fee00d7b769c651daa677ef84a7a84c1441a639.png)](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758711572000&to_ts=1758712772000&event_id=8295488853112891804&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:art-api-p01,service:mule** was **<= 0.0** at all times during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 11:14:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aart-api-p01&from_ts=1758711572000&to_ts=1758712772000&event_id=8295488853112891804&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View art-api-p01](/infrastructure?filter=art-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758712172000&to_ts=1758712592000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aart-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aart-api-p01&from_ts=1758712172000&to_ts=1758712472000&live=false&link_source=monitor_notif)] %%%",2025-09-24 11:14:32,"env:prod,host:art-api-p01,location:art_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",error
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 11:10:07,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:59:49,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:50:11,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:47:46,"env:prod,resource_hash:56a48e0cf0090f98,resource_name:get_/powershoutcurrency/expiringhours,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:47:46,"env:prod,resource_hash:37e8147c7593456,resource_name:get_/powershoutcurrency/bookings,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:47:46,"env:prod,resource_hash:318dcdf67a944fdd,resource_name:get_/powershoutcurrency/balance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:47:46,"env:prod,resource_hash:775824ba3089db26,resource_name:get_/drd/widget/powershout,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 5 different resources in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  [![Metric Graph]()](/watchdog/story/68bfbf18-a242-581f-b5a3-dba9087bdeea)  %%%,2025-09-24 10:47:46,"env:prod,resource_hash:84c02105c75767fb,resource_name:get_/powershoutcurrency/activity/history,service:mobile-api-service,source:watchdog,story_category:apm,story_key:68bfbf18-a242-581f-b5a3-dba9087bdeea,story_type:error_rate,team:ge-integration",info
Error rate increased on 4 different resources in mule,%%%   [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/fdaebb803517c00cff703b201c11b1ddaae7e6fd.png)](/watchdog/story/32ee468e-1d37-521c-83bc-fc561d6ba067)  %%%,2025-09-24 10:43:49,"env:prod,service:mule,source:watchdog,story_category:usm,story_key:32ee468e-1d37-521c-83bc-fc561d6ba067,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:40:19,"env:prod,resource_hash:c70754a8a38b84c1,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:40:19,"env:prod,resource_hash:73e61cc7f24f01a0,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:40:19,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:40:19,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"Error rate increased on 2 different resources in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:40:19,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:40:19,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:34:58,"env:prod,resource_hash:d14111fd312b973a,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
Error rate increased on 2 different resources in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  [![Metric Graph]()](/watchdog/story/3aef4852-c428-541b-8de3-0191fb72fb29)  %%%,2025-09-24 10:34:58,"env:prod,resource_hash:7b57e0b3ee661592,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3aef4852-c428-541b-8de3-0191fb72fb29,story_type:error_rate,team:ge-integration",info
"[P4] [Recovered on {container_name:U0mule-3-9-ee-esb-2,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-esb-2,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-esb-2%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435441461308669&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-esb-2%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-esb-2,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0nzx-sys-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0nzx-sys-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0nzx-sys-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435438181079690&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0nzx-sys-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0nzx-sys-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0gentrack-customers-sys-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0gentrack-customers-sys-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0gentrack-customers-sys-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435440907312357&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0gentrack-customers-sys-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0gentrack-customers-sys-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0customers-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0customers-exp-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0customers-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435443350837492&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0customers-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0customers-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-ee-genfsssupplypointupdatesync,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-genfsssupplypointupdatesync,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-genfsssupplypointupdatesync%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435445266190177&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-genfsssupplypointupdatesync%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-genfsssupplypointupdatesync,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0getloyaltyoffers,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0getloyaltyoffers,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0getloyaltyoffers%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435435659508926&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0getloyaltyoffers%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0getloyaltyoffers,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-ee-consumption,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-consumption,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-consumption%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435437063128341&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-consumption%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-consumption,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-ee-expr-1,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-expr-1,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-expr-1%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435440291539499&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-expr-1%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-expr-1,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-ee-expr-3,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-ee-expr-3,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-ee-expr-3%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435442963127092&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-ee-expr-3%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-4-4-0-ee-expr-3,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-fss-sync,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-fss-sync,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-fss-sync%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435439283387583&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-fss-sync%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-fss-sync,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0supplypointandmetering-esb-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0supplypointandmetering-esb-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0supplypointandmetering-esb-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435446123383038&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0supplypointandmetering-esb-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0supplypointandmetering-esb-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-ee-esb,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-ee-esb,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-ee-esb%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435445152296190&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-ee-esb%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-4-4-0-ee-esb,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-ee-esb-1,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-esb-1,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-esb-1%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435444489853749&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-esb-1%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-esb-1,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0work-orders,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0work-orders,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0work-orders%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435441320794355&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0work-orders%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0work-orders,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0billingaccounts-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0billingaccounts-exp-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0billingaccounts-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435436365670954&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0billingaccounts-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0billingaccounts-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-async-apps,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-async-apps,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-async-apps%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435443605216540&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-async-apps%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-4-4-0-async-apps,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-file-based,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-file-based,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-file-based%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435438593069745&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-file-based%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-4-4-0-file-based,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0loyaltyaccounts-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0loyaltyaccounts-exp-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0loyaltyaccounts-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435436491706206&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0loyaltyaccounts-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0loyaltyaccounts-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-loyaltyoffers,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-loyaltyoffers,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-loyaltyoffers%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435445141456699&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-loyaltyoffers%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-4-4-0-loyaltyoffers,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0supplypointandmetering-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0supplypointandmetering-exp-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0supplypointandmetering-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435440761939547&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0supplypointandmetering-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0supplypointandmetering-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-ee-b2bgateway,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-b2bgateway,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-b2bgateway%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435437809003515&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-b2bgateway%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-b2bgateway,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0powershout-eventbased,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0powershout-eventbased,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0powershout-eventbased%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435443877041552&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0powershout-eventbased%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0powershout-eventbased,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-ee-expr-1,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-ee-expr-1,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-ee-expr-1%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435441870468181&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-ee-expr-1%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-4-4-0-ee-expr-1,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0p2p_integration,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0p2p_integration,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0p2p_integration%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435440132332827&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0p2p_integration%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0p2p_integration,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-pricing,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-pricing,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-pricing%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435435703650659&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-pricing%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-4-4-0-pricing,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-ee-expr-3,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-expr-3,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-expr-3%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435444271209101&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-expr-3%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-expr-3,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0productandcontract-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0productandcontract-exp-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0productandcontract-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435439763140659&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0productandcontract-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0productandcontract-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0pricing-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0pricing-exp-api,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0pricing-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435439456148107&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0pricing-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0pricing-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-3-9-ee-esb-3,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-esb-3,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-esb-3%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435439180272306&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-esb-3%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0mule-3-9-ee-esb-3,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0docker-mule-3-9-events-pubsub,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0docker-mule-3-9-events-pubsub,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0docker-mule-3-9-events-pubsub%2Chost%3Aart-mule-u01&from_ts=1758708388000&to_ts=1758709588000&event_id=8295435436781130416&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708988000&to_ts=1758709408000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0docker-mule-3-9-events-pubsub%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708988000&to_ts=1758709288000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:21:28,"command:conmon,container_name:u0docker-mule-3-9-events-pubsub,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Recovered on {container_name:U0mule-4-4-0-loyaltyvouchers,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% [View matching processes](https://app.datadoghq.com/process?from_ts=1758708928000&to_ts=1758709348000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-loyaltyvouchers,env:uat,host:art-mule-u01** was **>= 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-loyaltyvouchers%2Chost%3Aart-mule-u01&from_ts=1758708328000&to_ts=1758709528000&event_id=8295434443325725194&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708928000&to_ts=1758709348000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-loyaltyvouchers%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708928000&to_ts=1758709228000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:20:28,"command:conmon,container_name:u0mule-4-4-0-loyaltyvouchers,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
"[P4] [Triggered on {container_name:U0mule-4-4-0-ee-expr-3,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-ee-expr-3 is not running! Impact Service that container U0mule-4-4-0-ee-expr-3 enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-ee-expr-3,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-ee-expr-3%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433427342867361&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-ee-expr-3%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-ee-expr-3,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-fss-sync,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-fss-sync is not running! Impact Service that container U0mule-3-9-fss-sync enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-fss-sync,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-fss-sync%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433423557733155&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-fss-sync%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-fss-sync,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-4-4-0-async-apps,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-async-apps is not running! Impact Service that container U0mule-4-4-0-async-apps enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-async-apps,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-async-apps%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433427429918319&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-async-apps%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-async-apps,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-esb-2,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-esb-2 is not running! Impact Service that container U0mule-3-9-ee-esb-2 enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-esb-2,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-esb-2%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433426319457184&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-esb-2%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-esb-2,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0supplypointandmetering-esb-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0supplypointandmetering-esb-api is not running! Impact Service that container U0supplypointandmetering-esb-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0supplypointandmetering-esb-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0supplypointandmetering-esb-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433429771743992&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0supplypointandmetering-esb-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0supplypointandmetering-esb-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-4-4-0-ee-expr-1,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-ee-expr-1 is not running! Impact Service that container U0mule-4-4-0-ee-expr-1 enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-ee-expr-1,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-ee-expr-1%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433426550518518&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-ee-expr-1%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-ee-expr-1,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0getloyaltyoffers,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0getloyaltyoffers is not running! Impact Service that container U0getloyaltyoffers enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0getloyaltyoffers,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0getloyaltyoffers%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433429472809530&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0getloyaltyoffers%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0getloyaltyoffers,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0customers-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0customers-exp-api is not running! Impact Service that container U0customers-exp-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0customers-exp-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0customers-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433427456488183&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0customers-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0customers-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-4-4-0-loyaltyoffers,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-loyaltyoffers is not running! Impact Service that container U0mule-4-4-0-loyaltyoffers enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-loyaltyoffers,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-loyaltyoffers%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433428141480961&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-loyaltyoffers%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-loyaltyoffers,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-consumption,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-consumption is not running! Impact Service that container U0mule-3-9-ee-consumption enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-consumption,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-consumption%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433431887040934&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-consumption%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-consumption,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-genfsssupplypointupdatesync,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-genfsssupplypointupdatesync is not running! Impact Service that container U0mule-3-9-ee-genfsssupplypointupdatesync enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-genfsssupplypointupdatesync,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-genfsssupplypointupdatesync%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433430158748096&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-genfsssupplypointupdatesync%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-genfsssupplypointupdatesync,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0powershout-eventbased,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0powershout-eventbased is not running! Impact Service that container U0powershout-eventbased enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0powershout-eventbased,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0powershout-eventbased%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433427185090351&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0powershout-eventbased%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0powershout-eventbased,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-esb-3,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-esb-3 is not running! Impact Service that container U0mule-3-9-ee-esb-3 enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-esb-3,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-esb-3%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433422935988789&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-esb-3%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-esb-3,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-expr-1,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-expr-1 is not running! Impact Service that container U0mule-3-9-ee-expr-1 enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-expr-1,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-expr-1%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433424928833467&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-expr-1%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-expr-1,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-b2bgateway,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-b2bgateway is not running! Impact Service that container U0mule-3-9-ee-b2bgateway enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-b2bgateway,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-b2bgateway%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433433394333626&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-b2bgateway%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-b2bgateway,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0productandcontract-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0productandcontract-exp-api is not running! Impact Service that container U0productandcontract-exp-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0productandcontract-exp-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0productandcontract-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433424583339733&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0productandcontract-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0productandcontract-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0gentrack-customers-sys-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0gentrack-customers-sys-api is not running! Impact Service that container U0gentrack-customers-sys-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0gentrack-customers-sys-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0gentrack-customers-sys-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433425566493469&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0gentrack-customers-sys-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0gentrack-customers-sys-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0supplypointandmetering-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0supplypointandmetering-exp-api is not running! Impact Service that container U0supplypointandmetering-exp-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0supplypointandmetering-exp-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0supplypointandmetering-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433425763171832&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0supplypointandmetering-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0supplypointandmetering-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-4-4-0-pricing,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-pricing is not running! Impact Service that container U0mule-4-4-0-pricing enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-pricing,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-pricing%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433429336904101&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-pricing%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-pricing,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-expr-3,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-expr-3 is not running! Impact Service that container U0mule-3-9-ee-expr-3 enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-expr-3,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-expr-3%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433428062191593&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-expr-3%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-expr-3,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0p2p_integration,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0p2p_integration is not running! Impact Service that container U0p2p_integration enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0p2p_integration,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0p2p_integration%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433424780132472&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0p2p_integration%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0p2p_integration,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
[P3] [Recovered on {http.status_code:503}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  **trace.express.request.errors** over **env:prod,http.status_code:503,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 10:10:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A503&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433430581222563&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A503&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"env:prod,http.status_code:503,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
"[P4] [Triggered on {container_name:U0nzx-sys-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0nzx-sys-api is not running! Impact Service that container U0nzx-sys-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0nzx-sys-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0nzx-sys-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433434189982494&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0nzx-sys-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0nzx-sys-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0pricing-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0pricing-exp-api is not running! Impact Service that container U0pricing-exp-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0pricing-exp-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0pricing-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433423197894157&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0pricing-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0pricing-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-4-4-0-ee-esb,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-ee-esb is not running! Impact Service that container U0mule-4-4-0-ee-esb enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-ee-esb,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-ee-esb%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433428435543350&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-ee-esb%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-ee-esb,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-4-4-0-file-based,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-file-based is not running! Impact Service that container U0mule-4-4-0-file-based enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-file-based,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-file-based%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433433039232102&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-file-based%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-file-based,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0work-orders,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0work-orders is not running! Impact Service that container U0work-orders enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0work-orders,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0work-orders%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433425370455575&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0work-orders%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0work-orders,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-4-4-0-loyaltyvouchers,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-4-4-0-loyaltyvouchers is not running! Impact Service that container U0mule-4-4-0-loyaltyvouchers enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-4-4-0-loyaltyvouchers,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-4-4-0-loyaltyvouchers%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433424195058080&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-4-4-0-loyaltyvouchers%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-4-4-0-loyaltyvouchers,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0loyaltyaccounts-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0loyaltyaccounts-exp-api is not running! Impact Service that container U0loyaltyaccounts-exp-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0loyaltyaccounts-exp-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0loyaltyaccounts-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433431322232432&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0loyaltyaccounts-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0loyaltyaccounts-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0mule-3-9-ee-esb-1,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0mule-3-9-ee-esb-1 is not running! Impact Service that container U0mule-3-9-ee-esb-1 enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0mule-3-9-ee-esb-1,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0mule-3-9-ee-esb-1%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433428463637573&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0mule-3-9-ee-esb-1%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0mule-3-9-ee-esb-1,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0docker-mule-3-9-events-pubsub,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0docker-mule-3-9-events-pubsub is not running! Impact Service that container U0docker-mule-3-9-events-pubsub enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0docker-mule-3-9-events-pubsub,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0docker-mule-3-9-events-pubsub%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433432689463219&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0docker-mule-3-9-events-pubsub%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0docker-mule-3-9-events-pubsub,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
"[P4] [Triggered on {container_name:U0billingaccounts-exp-api,host:art-mule-u01}] [Tier 3] UAT Mule Container is not running","%%% UAT Mule container U0billingaccounts-exp-api is not running! Impact Service that container U0billingaccounts-exp-api enables will only be running on one server. If this is a critical or busy service there are potential  impacts to upstream.    [View matching processes](https://app.datadoghq.com/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat) Number of processes matching **\*** on **command:conmon,container_name:U0billingaccounts-exp-api,env:uat,host:art-mule-u01** was **< 1** during the last **5m**   The monitor was last triggered at Wed Sep 24 2025 10:19:28 UTC.  - - -  [[Monitor Status](/monitors/212743288?group=container_name%3AU0billingaccounts-exp-api%2Chost%3Aart-mule-u01&from_ts=1758708268000&to_ts=1758709468000&event_id=8295433429830756224&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212743288/edit?link_source=monitor_notif)] · [[View art-mule-u01](/infrastructure?filter=art-mule-u01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758708868000&to_ts=1758709288000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&groups=container_name%2Chost&query=command%3Aconmon+env%3Auat&link_source=monitor_notif)] · [[Related Logs](/logs?query=command%3A%22conmon%22+container_name%3A%22U0billingaccounts-exp-api%22+env%3A%22uat%22+host%3A%22art-mule-u01%22&from_ts=1758708868000&to_ts=1758709168000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:19:28,"command:conmon,container_name:u0billingaccounts-exp-api,env:uat,host:art-mule-u01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
[P3] [Recovered on {http.status_code:502}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/6d40c4df87518ded777660e7dbf03a5382ff04b9.png)](/monitors/212107528?group=http.status_code%3A502&from_ts=1758708208000&to_ts=1758709408000&event_id=8295432425554390471&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:502,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 10:08:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A502&from_ts=1758708208000&to_ts=1758709408000&event_id=8295432425554390471&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A502&from_ts=1758708808000&to_ts=1758709108000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:18:28,"env:prod,http.status_code:502,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/59c0f94b7f33e520621aa8f5ec4ac0cbac29ad11.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758707848000&to_ts=1758709048000&event_id=8295426375402217334&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 10:00:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758707848000&to_ts=1758709048000&event_id=8295426375402217334&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758708448000&to_ts=1758708748000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:12:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:503}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 10:10:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/36d3b9fd1b829af3ceb66bb7373cb524ab66469a.png)](/monitors/212107528?group=http.status_code%3A503&from_ts=1758707728000&to_ts=1758708928000&event_id=8295424379051126944&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:503,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 10:10:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A503&from_ts=1758707728000&to_ts=1758708928000&event_id=8295424379051126944&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A503&from_ts=1758708328000&to_ts=1758708628000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:10:28,"env:prod,http.status_code:503,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {http.status_code:502}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 10:08:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/3cdffc855ec68241cca157f3a9e3ba8e2e3720bb.png)](/monitors/212107528?group=http.status_code%3A502&from_ts=1758707608000&to_ts=1758708808000&event_id=8295422360333179363&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:502,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 10:08:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A502&from_ts=1758707608000&to_ts=1758708808000&event_id=8295422360333179363&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A502&from_ts=1758708208000&to_ts=1758708508000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:08:28,"env:prod,http.status_code:502,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 10:00:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/19a2be9023a8a43244a5cb59429422f17de9ea34.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758707128000&to_ts=1758708328000&event_id=8295414308308930430&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 10:00:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758707128000&to_ts=1758708328000&event_id=8295414308308930430&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758707728000&to_ts=1758708028000&live=false&link_source=monitor_notif)] %%%",2025-09-24 10:00:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/841bd45e5ae4773189d53a60f394fb704c3eea8f.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758704939000&to_ts=1758706139000&event_id=8295378643974188340&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 08:57:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758704939000&to_ts=1758706139000&event_id=8295378643974188340&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758691439000&to_ts=1758705839000&live=false&link_source=monitor_notif)] %%%",2025-09-24 09:23:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/pricing/getsupplyagreementpricing/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/be1d649cd8ff9505be99b431ca6e238ec629aea2.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758704939000&to_ts=1758706139000&event_id=8295378643172355040&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 08:57:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758704939000&to_ts=1758706139000&event_id=8295378643172355040&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758691439000&to_ts=1758705839000&live=false&link_source=monitor_notif)] %%%",2025-09-24 09:23:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/pricing/getsupplyagreementpricing/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:post_/esb/pricing/getsupplyagreementpricing/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/pricing/getsupplyagreementpricing/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/958db4daee54a9b1588eb15ce4c7010e577071dc.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758703379000&to_ts=1758704579000&event_id=8295352511563161868&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 08:57:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758703379000&to_ts=1758704579000&event_id=8295352511563161868&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758689879000&to_ts=1758704279000&live=false&link_source=monitor_notif)] %%%",2025-09-24 08:57:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/pricing/getsupplyagreementpricing/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/productandcontract/supplyagreements/_/pricing.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/c631f0cba5f31002857cd12af26894b925c9efa7.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758703379000&to_ts=1758704579000&event_id=8295352510874948008&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 08:57:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758703379000&to_ts=1758704579000&event_id=8295352510874948008&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758689879000&to_ts=1758704279000&live=false&link_source=monitor_notif)] %%%",2025-09-24 08:57:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/795b1622c0c33e987462a0e1e87d74bf8914cf07.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758699059000&to_ts=1758700259000&event_id=8295280003040359930&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:05:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758699059000&to_ts=1758700259000&event_id=8295280003040359930&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758685559000&to_ts=1758699959000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:45:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/e50c710e856f69d3b2a4d8128a8ffa2c00eaa396.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758698639000&to_ts=1758699839000&event_id=8295273028889576266&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758698639000&to_ts=1758699839000&event_id=8295273028889576266&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758685139000&to_ts=1758699539000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:38:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/64eceb9c2fde227bf1717c7da924d5da50b1977b.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758698639000&to_ts=1758699839000&event_id=8295273028446397902&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758698639000&to_ts=1758699839000&event_id=8295273028446397902&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758685139000&to_ts=1758699539000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:38:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [Tier 1] Mule Prod API is reporting more errors than expected,%%%    @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/e20f5c630d4919ce9c958d7fd44826f15df2a085.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758698582000&to_ts=1758699782000&event_id=8295270929142174754&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:06:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758698582000&to_ts=1758699782000&event_id=8295270929142174754&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758698582000&to_ts=1758699482000&live=false&link_source=monitor_notif)] %%%,2025-09-24 07:38:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/811babde11a89315e4d18566888c3cab5441c7f5.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758698579000&to_ts=1758699779000&event_id=8295271940135784176&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758698579000&to_ts=1758699779000&event_id=8295271940135784176&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758685079000&to_ts=1758699479000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:37:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/productandcontract/creditextension/eligibility}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/d3ae49c58d4ca4244c2fe1f0467f40fd60e05abd.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758698579000&to_ts=1758699779000&event_id=8295271940623581721&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758698579000&to_ts=1758699779000&event_id=8295271940623581721&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758685079000&to_ts=1758699479000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:37:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/creditextension/eligibility,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/b6cf4175fe8f63fc73554c81fde3a7072e37f251.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758698068000&to_ts=1758699268000&event_id=8295262298494019024&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 07:15:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758698068000&to_ts=1758699268000&event_id=8295262298494019024&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758698668000&to_ts=1758698968000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:29:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/69a608f2326e47deaf292293da7b28a20bec927f.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758697802000&to_ts=1758699002000&event_id=8295257846279723699&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **<= 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:12:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758697802000&to_ts=1758699002000&event_id=8295257846279723699&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758697802000&to_ts=1758698822000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758697802000&to_ts=1758698702000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:25:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 07:15:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/4b482f8cc422b3d407001daf476fb27576eb04a0.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697228000&to_ts=1758698428000&event_id=8295248204052463218&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 07:15:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697228000&to_ts=1758698428000&event_id=8295248204052463218&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758697828000&to_ts=1758698128000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:15:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/fd012a10e00532d3eb7de738e36ab3f54995fd7c.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697168000&to_ts=1758698368000&event_id=8295247204429099162&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 07:13:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697168000&to_ts=1758698368000&event_id=8295247204429099162&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758697768000&to_ts=1758698068000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:14:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 07:13:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/9d5b8104a9613710b133bd757664b9e5571fa7c6.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697108000&to_ts=1758698308000&event_id=8295246200001785323&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 07:13:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697108000&to_ts=1758698308000&event_id=8295246200001785323&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758697708000&to_ts=1758698008000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:13:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/4be0c9cfd08c3b4edf4314ed61daeeabdf73b939.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697048000&to_ts=1758698248000&event_id=8295245187789420246&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 06:53:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758697048000&to_ts=1758698248000&event_id=8295245187789420246&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758697648000&to_ts=1758697948000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:12:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%% High 5-minute load average detected on host prt-mule-p02.genesispower.co.nz.  This alert means the system has more processes waiting to run than it can efficiently handle. A high load average often indicates that the host is under significant resource pressure — typically due to CPU saturation, blocked I/O, or memory contention. While some load is normal, consistently high load (especially relative to the number of CPU cores) can signal an unhealthy or overloaded system.  --- ## Impact  If load remains high, the host may experience:  - Slower system and application performance - Backlogged scheduled jobs or delayed processing - Increased risk of timeouts, dropped requests, or service degradation.   High load can quickly lead to cascading failures or degraded user experience for upstream services EIQ and frontend and downstream services including Gentrack.   ### Initial troubleshooting  1. Identify the affected host from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for prt-mule-p02.genesispower.co.nz. 3. In the **Metrics tab**, examine:    - `system.load.1`    - `system.cpu.user`, `system.cpu.system`, `system.io.await`, `system.mem.pct_usable` 4. Use [**Live Processes**](https://app.datadoghq.com/infrastructure/processes) to find CPU- or I/O-heavy processes.              ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3Aprt-mule-p02.genesispower.co.nz) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/883b85b55129b340b969581f40b270cd43f86871.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758697022000&to_ts=1758698222000&event_id=8295244752972529179&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **> 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:12:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758697022000&to_ts=1758698222000&event_id=8295244752972529179&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758697022000&to_ts=1758698042000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758697022000&to_ts=1758697922000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:12:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/productandcontract/validatecreditextensioneligibility/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/bf4d71a5111a0a00f70c9d7ff3f0b97d2a4d5d24.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758696959000&to_ts=1758698159000&event_id=8295244880163963094&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758696959000&to_ts=1758698159000&event_id=8295244880163963094&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758683459000&to_ts=1758697859000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:10:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/productandcontract/creditextension/eligibility}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/productandcontract/creditextension/eligibility.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/8c91669d5556e5b0dd7eea88bdd4ae3426d9f874.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758696779000&to_ts=1758697979000&event_id=8295241754454272057&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758696779000&to_ts=1758697979000&event_id=8295241754454272057&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758683279000&to_ts=1758697679000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:07:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/creditextension/eligibility,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/supplypointandmetering/devices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/b39c70b3255e3b50bfe95f59617dcf13626b3f60.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758696779000&to_ts=1758697979000&event_id=8295241754381749684&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758696779000&to_ts=1758697979000&event_id=8295241754381749684&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758683279000&to_ts=1758697679000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:07:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/supplypointandmetering/consumption.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/3d4ac4750acf9af19b49c0d8d8315b5ddb1f7005.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758696779000&to_ts=1758697979000&event_id=8295241753817536655&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758696779000&to_ts=1758697979000&event_id=8295241753817536655&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758683279000&to_ts=1758697679000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:07:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [Tier 1] Mule Prod API is reporting more errors than expected,%%% Mule Prod  API get_/v1/exp/supplypointandmetering/devices is reporting higher  error rate than expected     @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/31cca273d248be796da5cee72198d2048bb6c854.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758696662000&to_ts=1758697862000&event_id=8295238708687137105&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:06:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758696662000&to_ts=1758697862000&event_id=8295238708687137105&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758696662000&to_ts=1758697562000&live=false&link_source=monitor_notif)] %%%,2025-09-24 07:06:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/expr/financial/accountevent/invoices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/dd23fb24313c7153c4acace61944132695b15a22.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758696659000&to_ts=1758697859000&event_id=8295239726905246147&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 07:05:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758696659000&to_ts=1758697859000&event_id=8295239726905246147&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758683159000&to_ts=1758697559000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:05:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {host:PRT-LOGSTASH-P01.genesispower.co.nz}] [Tier 1] Mule Prod Host  is experiencing  slow disk writes,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/710d8513aa5abcce00fecb5674d6ad8ab1454744.png)](/monitors/204497467?group=host%3APRT-LOGSTASH-P01.genesispower.co.nz&from_ts=1758696607000&to_ts=1758697807000&event_id=8295237800486103023&link_source=monitor_notif)  At least **100%** of **`avg:system.disk.write_time{env:prod,service:mule}.as_count()`** values have been more than **2** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 06:40:07 UTC.  - - -  [[Monitor Status](/monitors/204497467?group=host%3APRT-LOGSTASH-P01.genesispower.co.nz&from_ts=1758696607000&to_ts=1758697807000&event_id=8295237800486103023&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204497467/edit?link_source=monitor_notif)] · [[View PRT-LOGSTASH-P01.genesispower.co.nz](/infrastructure?filter=PRT-LOGSTASH-P01.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758683107000&to_ts=1758697627000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&query=host%3APRT-LOGSTASH-P01.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2C+service%3Amule%29+AND+host%3APRT-LOGSTASH-P01.genesispower.co.nz&from_ts=1758683107000&to_ts=1758697507000&live=false&link_source=monitor_notif)] %%%",2025-09-24 07:05:07,"env:prod,host:prt-logstash-p01.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 06:53:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/cf9860ba69ab418b5f2f63471705075072ae58fb.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758695908000&to_ts=1758697108000&event_id=8295226061247750843&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 06:53:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758695908000&to_ts=1758697108000&event_id=8295226061247750843&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758696508000&to_ts=1758696808000&live=false&link_source=monitor_notif)] %%%",2025-09-24 06:53:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/61032d5140deebdd57168c18adea61951a7145d7.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758695548000&to_ts=1758696748000&event_id=8295220023060674706&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 06:43:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758695548000&to_ts=1758696748000&event_id=8295220023060674706&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758696148000&to_ts=1758696448000&live=false&link_source=monitor_notif)] %%%",2025-09-24 06:47:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 06:43:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/a185e62ea77c4ac65362284978e4a72d9774ed24.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758695308000&to_ts=1758696508000&event_id=8295215995377630310&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 06:43:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758695308000&to_ts=1758696508000&event_id=8295215995377630310&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758695908000&to_ts=1758696208000&live=false&link_source=monitor_notif)] %%%",2025-09-24 06:43:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {host:PRT-LOGSTASH-P01.genesispower.co.nz}] [Tier 1] Mule Prod Host  is experiencing  slow disk writes,"%%%  ### Slow disk writes detected on host PRT-LOGSTASH-P01.genesispower.co.nz.  ## Impact  - Slower application and system performance, especially for disk-intensive services - Delayed job processing or database queries - Risk of timeouts or service degradation due to blocked I/O operations  Sustained disk latency can have cascading effects on dependent services (EIQ, Gentrack) and degrade user experience.  ###  Initial troubleshooting  1. Identify the affected host and device from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for PRT-LOGSTASH-P01.genesispower.co.nz. 3. In the **Metrics tab**, examine:    - `system.io.await` by device    - `system.disk.read_time`, `system.disk.write_time`, `system.disk.in_use` 4. Use [**Live Processes**](https://app.datadoghq.com/processes) to find I/O-heavy processes. These hosts are managed by Spark  Assign ticket to Integration team           ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3APRT-LOGSTASH-P01.genesispower.co.nz) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/b6993ad5c132dbc588649df29dfdbebfee20f273.png)](/monitors/204497467?group=host%3APRT-LOGSTASH-P01.genesispower.co.nz&from_ts=1758695107000&to_ts=1758696307000&event_id=8295212625469465877&link_source=monitor_notif)  At least **100%** of **`avg:system.disk.write_time{env:prod,service:mule}.as_count()`** values have been more than **2** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 06:40:07 UTC.  - - -  [[Monitor Status](/monitors/204497467?group=host%3APRT-LOGSTASH-P01.genesispower.co.nz&from_ts=1758695107000&to_ts=1758696307000&event_id=8295212625469465877&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204497467/edit?link_source=monitor_notif)] · [[View PRT-LOGSTASH-P01.genesispower.co.nz](/infrastructure?filter=PRT-LOGSTASH-P01.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758681607000&to_ts=1758696127000&live=false&showSummaryGraphs=true&sort=memory%2CDESC&query=host%3APRT-LOGSTASH-P01.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2C+service%3Amule%29+AND+host%3APRT-LOGSTASH-P01.genesispower.co.nz&from_ts=1758681607000&to_ts=1758696007000&live=false&link_source=monitor_notif)] %%%",2025-09-24 06:40:07,"env:prod,host:prt-logstash-p01.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [Tier 1] Mule Prod API is reporting more errors than expected,%%%    @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/c75738043b743106bb52079503b7043533950177.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758693002000&to_ts=1758694202000&event_id=8295177315852041190&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:44:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758693002000&to_ts=1758694202000&event_id=8295177315852041190&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758693002000&to_ts=1758693902000&live=false&link_source=monitor_notif)] %%%,2025-09-24 06:05:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P5] [Recovered on {container_name:u0financial-exp-api}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%% [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/ecb3a531e33ac950749f6a248f8fef374267d2d9.png)](/monitors/204522115?group=container_name%3Au0financial-exp-api&from_ts=1758692875000&to_ts=1758694075000&event_id=8295175196213087776&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:45:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0financial-exp-api&from_ts=1758692875000&to_ts=1758694075000&event_id=8295175196213087776&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0financial-exp-api&from_ts=1758679375000&to_ts=1758693775000&live=false&link_source=monitor_notif)] %%%",2025-09-24 06:02:55,"container_name:u0financial-exp-api,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",success
[P5] [Recovered on {container_name:u0payments-exp-api}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%% [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/aa946bd2dd39d6d6bc30123e1c644872a2e00487.png)](/monitors/204522115?group=container_name%3Au0payments-exp-api&from_ts=1758692695000&to_ts=1758693895000&event_id=8295172184849161410&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:45:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0payments-exp-api&from_ts=1758692695000&to_ts=1758693895000&event_id=8295172184849161410&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0payments-exp-api&from_ts=1758679195000&to_ts=1758693595000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:59:55,"container_name:u0payments-exp-api,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",success
[P5] [Recovered on {container_name:u0mule-4-4-0-powershout-currency}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%% [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/9f35725e48fd93d4638f4636c514cf8c8afd1ad5.png)](/monitors/204522115?group=container_name%3Au0mule-4-4-0-powershout-currency&from_ts=1758692695000&to_ts=1758693895000&event_id=8295172186746804888&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:46:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0mule-4-4-0-powershout-currency&from_ts=1758692695000&to_ts=1758693895000&event_id=8295172186746804888&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0mule-4-4-0-powershout-currency&from_ts=1758679195000&to_ts=1758693595000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:59:55,"container_name:u0mule-4-4-0-powershout-currency,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",success
[P5] [Recovered on {container_name:u0mule-3-9-ee-expr-2}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%% [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/9dc1d269532e0cbcf05509f4e0b106a44147cbe7.png)](/monitors/204522115?group=container_name%3Au0mule-3-9-ee-expr-2&from_ts=1758692575000&to_ts=1758693775000&event_id=8295170164072524774&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:47:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0mule-3-9-ee-expr-2&from_ts=1758692575000&to_ts=1758693775000&event_id=8295170164072524774&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0mule-3-9-ee-expr-2&from_ts=1758679075000&to_ts=1758693475000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:57:55,"container_name:u0mule-3-9-ee-expr-2,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",success
[P4] [Recovered on {host:DCB-ESB-T01}] [Tier 3] Mule Test - System load is high for host,"%%% [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/476aef8007f99560527fb89858f1052d0a73a607.png)](/monitors/179908515?group=host%3ADCB-ESB-T01&from_ts=1758692475000&to_ts=1758693675000&event_id=8295168469606771948&link_source=monitor_notif)  **system.load.5** over **env:test,host:DCB-ESB-T01,service:mule** was **<= 275.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 05:46:15 UTC.  - - -  [[Monitor Status](/monitors/179908515?group=host%3ADCB-ESB-T01&from_ts=1758692475000&to_ts=1758693675000&event_id=8295168469606771948&link_source=monitor_notif)] · [[Edit Monitor](/monitors/179908515/edit?link_source=monitor_notif)] · [[View DCB-ESB-T01](/infrastructure?filter=DCB-ESB-T01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758692475000&to_ts=1758693495000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3ADCB-ESB-T01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Atest+%2Cservice%3Amule%29+AND+host%3ADCB-ESB-T01&from_ts=1758692475000&to_ts=1758693375000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:56:15,"env:test,host:dcb-esb-t01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",success
[P5] [Triggered on {container_name:u0mule-3-9-ee-expr-2}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%%  Mule UAT container u0mule-3-9-ee-expr-2 CPU is abnormal for this time.   Check container status   [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/a8aa14ad015067042f010370fcd019219d9ef8e8.png)](/monitors/204522115?group=container_name%3Au0mule-3-9-ee-expr-2&from_ts=1758691975000&to_ts=1758693175000&event_id=8295160085565804608&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:47:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0mule-3-9-ee-expr-2&from_ts=1758691975000&to_ts=1758693175000&event_id=8295160085565804608&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0mule-3-9-ee-expr-2&from_ts=1758678475000&to_ts=1758692875000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:47:55,"container_name:u0mule-3-9-ee-expr-2,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",error
[P5] [Triggered on {container_name:u0mule-4-4-0-powershout-currency}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%%  Mule UAT container u0mule-4-4-0-powershout-currency CPU is abnormal for this time.   Check container status   [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/7e2fcb2558cc4e8eac72b0da13c74a6aec4e57da.png)](/monitors/204522115?group=container_name%3Au0mule-4-4-0-powershout-currency&from_ts=1758691915000&to_ts=1758693115000&event_id=8295159076617434396&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:46:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0mule-4-4-0-powershout-currency&from_ts=1758691915000&to_ts=1758693115000&event_id=8295159076617434396&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0mule-4-4-0-powershout-currency&from_ts=1758678415000&to_ts=1758692815000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:46:55,"container_name:u0mule-4-4-0-powershout-currency,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",error
[P4] [Triggered on {host:DCB-ESB-T01}] [Tier 3] Mule Test - System load is high for host,"%%% High 5-minute load average detected on host DCB-ESB-T01.  This alert means the system has more processes waiting to run than it can efficiently handle. A high load average often indicates that the host is under significant resource pressure — typically due to CPU saturation, blocked I/O, or memory contention. While some load is normal, consistently high load (especially relative to the number of CPU cores) can signal an unhealthy or overloaded system.  --- ## Impact  If load remains high, the host may experience:  - Slower system and application performance - Backlogged scheduled jobs or delayed processing - Increased risk of timeouts, dropped requests, or service degradation.   High load can quickly lead to cascading failures or degraded user experience for upstream services EIQ and frontend and downstream services including Gentrack.   ### Initial troubleshooting  1. Identify the affected host from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for DCB-ESB-T01. 3. In the **Metrics tab**, examine:    - `system.load.1`    - `system.cpu.user`, `system.cpu.system`, `system.io.await`, `system.mem.pct_usable` 4. Use [**Live Processes**](https://app.datadoghq.com/infrastructure/processes) to find CPU- or I/O-heavy processes.              ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3ADCB-ESB-T01) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)   [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/acacd27a37f7a0839d176d357668ca37bf252308.png)](/monitors/179908515?group=host%3ADCB-ESB-T01&from_ts=1758691875000&to_ts=1758693075000&event_id=8295158396150107396&link_source=monitor_notif)  **system.load.5** over **env:test,host:DCB-ESB-T01,service:mule** was **> 275.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 05:46:15 UTC.  - - -  [[Monitor Status](/monitors/179908515?group=host%3ADCB-ESB-T01&from_ts=1758691875000&to_ts=1758693075000&event_id=8295158396150107396&link_source=monitor_notif)] · [[Edit Monitor](/monitors/179908515/edit?link_source=monitor_notif)] · [[View DCB-ESB-T01](/infrastructure?filter=DCB-ESB-T01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758691875000&to_ts=1758692895000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3ADCB-ESB-T01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Atest+%2Cservice%3Amule%29+AND+host%3ADCB-ESB-T01&from_ts=1758691875000&to_ts=1758692775000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:46:15,"env:test,host:dcb-esb-t01,location:art_datacentre,monitor,priority:p4,service:mule,source:alert,team:ge-integration,tier:tier_3",error
[P5] [Triggered on {container_name:u0payments-exp-api}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%%  Mule UAT container u0payments-exp-api CPU is abnormal for this time.   Check container status   [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/9625106109c492f1a23637580323d726c2bc2b67.png)](/monitors/204522115?group=container_name%3Au0payments-exp-api&from_ts=1758691855000&to_ts=1758693055000&event_id=8295158094855645074&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:45:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0payments-exp-api&from_ts=1758691855000&to_ts=1758693055000&event_id=8295158094855645074&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0payments-exp-api&from_ts=1758678355000&to_ts=1758692755000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:45:55,"container_name:u0payments-exp-api,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",error
[P5] [Triggered on {container_name:u0financial-exp-api}] [Tier 3 ] Mule UAT  anomalogous alert for container CPU usage.,"%%%  Mule UAT container u0financial-exp-api CPU is abnormal for this time.   Check container status   [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/3a000afda61593f9baa92bbc02a5b16cfce96927.png)](/monitors/204522115?group=container_name%3Au0financial-exp-api&from_ts=1758691855000&to_ts=1758693055000&event_id=8295158094213089387&link_source=monitor_notif)  At least **100%** of **`sum:container.cpu.usage{env:uat,service:mule}`** values have been more than **3** deviations **above** the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 05:45:55 UTC.  - - -  [[Monitor Status](/monitors/204522115?group=container_name%3Au0financial-exp-api&from_ts=1758691855000&to_ts=1758693055000&event_id=8295158094213089387&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204522115/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Auat+%2Cservice%3Amule%29+AND+container_name%3Au0financial-exp-api&from_ts=1758678355000&to_ts=1758692755000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:45:55,"container_name:u0financial-exp-api,env:uat,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/b63dc8e4857eb12efb5bbbc536c926e0785cb678.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758689548000&to_ts=1758690748000&event_id=8295119358584214161&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 05:02:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758689548000&to_ts=1758690748000&event_id=8295119358584214161&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758690148000&to_ts=1758690448000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:07:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 05:02:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/9a6e69644746203ad728d332170cacb985af870c.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758689248000&to_ts=1758690448000&event_id=8295114334198246080&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 05:02:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758689248000&to_ts=1758690448000&event_id=8295114334198246080&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758689848000&to_ts=1758690148000&live=false&link_source=monitor_notif)] %%%",2025-09-24 05:02:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/c6e63647ef05fd99647a9890805d069b614a539a.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758688019000&to_ts=1758689219000&event_id=8295094779725719293&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:17:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758688019000&to_ts=1758689219000&event_id=8295094779725719293&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758674519000&to_ts=1758688919000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:41:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/productandcontract/creditextension/eligibility}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/a4365fa276ce6ea02fe362ef3c1f5d9b8b11500c.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758687839000&to_ts=1758689039000&event_id=8295091752150590543&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:15:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758687839000&to_ts=1758689039000&event_id=8295091752150590543&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758674339000&to_ts=1758688739000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:38:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/creditextension/eligibility,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/ccfa07b132ef5ac1d8cd84bebab5430d1a2b10db.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758687839000&to_ts=1758689039000&event_id=8295091752953126657&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:15:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758687839000&to_ts=1758689039000&event_id=8295091752953126657&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758674339000&to_ts=1758688739000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:38:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/35241cd0f3787ba95b50c01883a48f035c66e46f.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758687539000&to_ts=1758688739000&event_id=8295086733211358838&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758687539000&to_ts=1758688739000&event_id=8295086733211358838&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758674039000&to_ts=1758688439000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:33:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/f681fa1f720e1cc4f1c6ec0d98614a9243a6df14.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758687482000&to_ts=1758688682000&event_id=8295084695790486275&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **<= 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 04:10:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758687482000&to_ts=1758688682000&event_id=8295084695790486275&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758687482000&to_ts=1758688502000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758687482000&to_ts=1758688382000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:33:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/91bb40a226cdcc7ddae382a901434c9985d75786.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758687479000&to_ts=1758688679000&event_id=8295085714324911502&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 04:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758687479000&to_ts=1758688679000&event_id=8295085714324911502&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758673979000&to_ts=1758688379000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:32:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/financial/accountevents/transactions}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/20076e67a291d8d592c5c2ec11da9db4b311b1e1.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758687419000&to_ts=1758688619000&event_id=8295084712158045977&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:20:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758687419000&to_ts=1758688619000&event_id=8295084712158045977&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758673919000&to_ts=1758688319000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:31:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/financial/accountevents/transactions,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/8fd0a4e85b6ba71e1a6b94f2c535df8c3f8105f2.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758687028000&to_ts=1758688228000&event_id=8295077077495777557&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 03:47:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758687028000&to_ts=1758688228000&event_id=8295077077495777557&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758687628000&to_ts=1758687928000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:25:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/supplypointandmetering/devices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/4add667cc63b8279f548a305ac8d2f4b13832727.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758686159000&to_ts=1758687359000&event_id=8295063592764950738&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 04:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758686159000&to_ts=1758687359000&event_id=8295063592764950738&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758672659000&to_ts=1758687059000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:10:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%% High 5-minute load average detected on host prt-mule-p02.genesispower.co.nz.  This alert means the system has more processes waiting to run than it can efficiently handle. A high load average often indicates that the host is under significant resource pressure — typically due to CPU saturation, blocked I/O, or memory contention. While some load is normal, consistently high load (especially relative to the number of CPU cores) can signal an unhealthy or overloaded system.  --- ## Impact  If load remains high, the host may experience:  - Slower system and application performance - Backlogged scheduled jobs or delayed processing - Increased risk of timeouts, dropped requests, or service degradation.   High load can quickly lead to cascading failures or degraded user experience for upstream services EIQ and frontend and downstream services including Gentrack.   ### Initial troubleshooting  1. Identify the affected host from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for prt-mule-p02.genesispower.co.nz. 3. In the **Metrics tab**, examine:    - `system.load.1`    - `system.cpu.user`, `system.cpu.system`, `system.io.await`, `system.mem.pct_usable` 4. Use [**Live Processes**](https://app.datadoghq.com/infrastructure/processes) to find CPU- or I/O-heavy processes.              ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3Aprt-mule-p02.genesispower.co.nz) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/88c5d8eda6d952e20b3db0dd271f2e2f75e6a171.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758686102000&to_ts=1758687302000&event_id=8295061546690777715&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **> 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 04:10:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758686102000&to_ts=1758687302000&event_id=8295061546690777715&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758686102000&to_ts=1758687122000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758686102000&to_ts=1758687002000&live=false&link_source=monitor_notif)] %%%",2025-09-24 04:10:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/eb30d234baaf6ce789fe173e1a8e462a2ceab078.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758685019000&to_ts=1758686219000&event_id=8295044448782361078&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758685019000&to_ts=1758686219000&event_id=8295044448782361078&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758671519000&to_ts=1758685919000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:51:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/bb57397d2434def39bf9d9e8691be32109feb120.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758684842000&to_ts=1758686042000&event_id=8295040412889005315&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **<= 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758684842000&to_ts=1758686042000&event_id=8295040412889005315&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758684842000&to_ts=1758685862000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758684842000&to_ts=1758685742000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:49:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/expr/billingaccounts/v1/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/c852f4faa821780662b7915a5ea20689cda50078.png)](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758684779000&to_ts=1758685979000&event_id=8295040489717473577&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758684779000&to_ts=1758685979000&event_id=8295040489717473577&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758671279000&to_ts=1758685679000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:47:59,"env:prod,monitor,priority:p3,resource_name:get_/expr/billingaccounts/v1/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/372ab17bb9babf450b8751b7496dbeaf90936f14.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758684779000&to_ts=1758685979000&event_id=8295040489530711367&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:26:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758684779000&to_ts=1758685979000&event_id=8295040489530711367&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758671279000&to_ts=1758685679000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:47:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 03:47:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/97f73eb3390e2a97df89bb0131185b244b795ae7.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758684748000&to_ts=1758685948000&event_id=8295038830312967506&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 03:47:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758684748000&to_ts=1758685948000&event_id=8295038830312967506&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758685348000&to_ts=1758685648000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:47:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/bd149f1389ab47379d1887cc6ccec56129ec7193.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758684688000&to_ts=1758685888000&event_id=8295037827527340235&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 03:14:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758684688000&to_ts=1758685888000&event_id=8295037827527340235&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758685288000&to_ts=1758685588000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:46:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-billingaccounts/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/efa6f5abc6b2ec4eaaa4d5f90a57d0d66e32bc69.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758683519000&to_ts=1758684719000&event_id=8295019285113103068&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:26:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758683519000&to_ts=1758684719000&event_id=8295019285113103068&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758670019000&to_ts=1758684419000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:26:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/financial/accountevents/transactions}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/financial/accountevents/transactions.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/d0cae045d7ab1c7e1a419e415b7e1dc7c01c2f98.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758683159000&to_ts=1758684359000&event_id=8295013245954127834&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:20:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758683159000&to_ts=1758684359000&event_id=8295013245954127834&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758669659000&to_ts=1758684059000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:20:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/financial/accountevents/transactions,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/expr/financial/accountevent/invoices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/4cc1db6d4cb356724c3aac9b51f81732880efadb.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758682979000&to_ts=1758684179000&event_id=8295010222737912670&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:17:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758682979000&to_ts=1758684179000&event_id=8295010222737912670&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758669479000&to_ts=1758683879000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:17:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/productandcontract/creditextension/eligibility}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/productandcontract/creditextension/eligibility.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/92472be9eb6527b1a0716b14708563a63f0e2980.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758682859000&to_ts=1758684059000&event_id=8295008299859299237&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:15:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758682859000&to_ts=1758684059000&event_id=8295008299859299237&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fcreditextension%2Feligibility&from_ts=1758669359000&to_ts=1758683759000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:15:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/creditextension/eligibility,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/productandcontract/validatecreditextensioneligibility/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/8dc5cbd6b54873913a3b2836c0fca8cecac784b0.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758682859000&to_ts=1758684059000&event_id=8295008300307648724&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:15:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758682859000&to_ts=1758684059000&event_id=8295008300307648724&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fvalidatecreditextensioneligibility%2Fv1&from_ts=1758669359000&to_ts=1758683759000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:15:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/productandcontract/validatecreditextensioneligibility/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 03:14:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/25b0b3914950f10b4fd8640a9f9b2ce91d113b72.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758682768000&to_ts=1758683968000&event_id=8295005613377617655&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 03:14:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758682768000&to_ts=1758683968000&event_id=8295005613377617655&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758683368000&to_ts=1758683668000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:14:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  The monitor was marked as **Recovered** on **env:prod,http.status_code:500,service:esb-connector-service** by jacqui.rennie@ddsit.co.nz.  The monitor was last triggered at Wed Sep 24 2025 03:00:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758682741000&to_ts=1758683941000&event_id=8295004153789562155&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758683341000&to_ts=1758683641000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:14:01,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/expr/billingaccounts/v1/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/expr/billingaccounts/v1/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/23083ee1dae5c814364efe04a7fd1a63735971ad.png)](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758682679000&to_ts=1758683879000&event_id=8295005177217786008&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758682679000&to_ts=1758683879000&event_id=8295005177217786008&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758669179000&to_ts=1758683579000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:12:59,"env:prod,monitor,priority:p3,resource_name:get_/expr/billingaccounts/v1/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/supplypointandmetering/devices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/dfb5df44efb7708973b347791b0a67fdf5b9d303.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758682679000&to_ts=1758683879000&event_id=8295005177157113244&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758682679000&to_ts=1758683879000&event_id=8295005177157113244&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758669179000&to_ts=1758683579000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:12:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/supplypointandmetering/consumption.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/eae7a03b4e699f7ee6f96321a7ce9c4318887c53.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758682679000&to_ts=1758683879000&event_id=8295005177154627681&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758682679000&to_ts=1758683879000&event_id=8295005177154627681&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758669179000&to_ts=1758683579000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:12:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%% High 5-minute load average detected on host prt-mule-p02.genesispower.co.nz.  This alert means the system has more processes waiting to run than it can efficiently handle. A high load average often indicates that the host is under significant resource pressure — typically due to CPU saturation, blocked I/O, or memory contention. While some load is normal, consistently high load (especially relative to the number of CPU cores) can signal an unhealthy or overloaded system.  --- ## Impact  If load remains high, the host may experience:  - Slower system and application performance - Backlogged scheduled jobs or delayed processing - Increased risk of timeouts, dropped requests, or service degradation.   High load can quickly lead to cascading failures or degraded user experience for upstream services EIQ and frontend and downstream services including Gentrack.   ### Initial troubleshooting  1. Identify the affected host from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for prt-mule-p02.genesispower.co.nz. 3. In the **Metrics tab**, examine:    - `system.load.1`    - `system.cpu.user`, `system.cpu.system`, `system.io.await`, `system.mem.pct_usable` 4. Use [**Live Processes**](https://app.datadoghq.com/infrastructure/processes) to find CPU- or I/O-heavy processes.              ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3Aprt-mule-p02.genesispower.co.nz) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/af1bf0e9ddb084c738f6f9a67c6476696c304293.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758682622000&to_ts=1758683822000&event_id=8295003159861255433&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **> 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 03:12:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758682622000&to_ts=1758683822000&event_id=8295003159861255433&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758682622000&to_ts=1758683642000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758682622000&to_ts=1758683522000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:12:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 03:10:41,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 03:10:41,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 03:00:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/abf42277095b0455f22ec56b875a1acccba7714b.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758681928000&to_ts=1758683128000&event_id=8294991510634740066&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 03:00:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758681928000&to_ts=1758683128000&event_id=8294991510634740066&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758682528000&to_ts=1758682828000&live=false&link_source=monitor_notif)] %%%",2025-09-24 03:00:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 03:00:13,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 03:00:13,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 03:00:13,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:50:06,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:50:06,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 02:50:06,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 02:39:48,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 02:30:41,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
[P3] [Recovered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/7dcd982040e918032b4a378012e8773bf809a435.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758679739000&to_ts=1758680939000&event_id=8294955890779522247&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 02:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758679739000&to_ts=1758680939000&event_id=8294955890779522247&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758666239000&to_ts=1758680639000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:23:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/d775744cbc2fa6258ae81a220d30f75a9cabb4f6.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758679708000&to_ts=1758680908000&event_id=8294954270378362014&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 02:19:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758679708000&to_ts=1758680908000&event_id=8294954270378362014&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758680308000&to_ts=1758680608000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:23:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:patch_/v1/exp/customers/payments/result/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/3a4884c48dd3d75b5e8ed04a686bb82d93aef2f9.png)](/monitors/213661559?group=resource_name%3Apatch_%2Fv1%2Fexp%2Fcustomers%2Fpayments%2Fresult%2F&from_ts=1758679619000&to_ts=1758680819000&event_id=8294953933095430771&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:19:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apatch_%2Fv1%2Fexp%2Fcustomers%2Fpayments%2Fresult%2F&from_ts=1758679619000&to_ts=1758680819000&event_id=8294953933095430771&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apatch_%2Fv1%2Fexp%2Fcustomers%2Fpayments%2Fresult%2F&from_ts=1758666119000&to_ts=1758680519000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:21:59,"env:prod,monitor,priority:p3,resource_name:patch_/v1/exp/customers/payments/result/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 02:20:31,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 02:19:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/465d4d874d0d8a458dd0adb2816338c1bd723cca.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758679468000&to_ts=1758680668000&event_id=8294950255733085534&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 02:19:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758679468000&to_ts=1758680668000&event_id=8294950255733085534&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758680068000&to_ts=1758680368000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:19:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/940a59dbaefafc0ec4687cddfcf1b812a0cfd868.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758678988000&to_ts=1758680188000&event_id=8294942196701660952&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 02:03:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758678988000&to_ts=1758680188000&event_id=8294942196701660952&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758679588000&to_ts=1758679888000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:11:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 02:10:03,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
[P3] [Recovered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/3a647f1808ece55ad56f25b1ef9e4744552a7e21.png)](/monitors/204752676?from_ts=1758678876000&to_ts=1758680076000&event_id=8294941332999447423&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 01:17:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758678876000&to_ts=1758680076000&event_id=8294941332999447423&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758665376000&to_ts=1758679776000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:09:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/expr/financial/accountevent/invoices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/244ede8194b4fa5d073239113f802a13d585fed6.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758678779000&to_ts=1758679979000&event_id=8294939795205779777&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 02:07:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758678779000&to_ts=1758679979000&event_id=8294939795205779777&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758665279000&to_ts=1758679679000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:07:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 02:03:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/dabe0eb089a2b7793d36e32d3d0abce8722fa750.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758678508000&to_ts=1758679708000&event_id=8294934144528434080&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 02:03:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758678508000&to_ts=1758679708000&event_id=8294934144528434080&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758679108000&to_ts=1758679408000&live=false&link_source=monitor_notif)] %%%",2025-09-24 02:03:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:59:50,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:59:50,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:59:50,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:59:50,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:59:50,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 01:59:50,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 01:59:50,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 01:59:49,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 01:50:04,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
Error rate increased on the POST /powershoutcurrency/booking/delete resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  [![Metric Graph]()](/watchdog/story/6187e3d6-b9cd-50b0-a17c-dd501115a16d)  %%%,2025-09-24 01:48:34,"env:prod,resource_hash:ac7acc6097c0acd7,resource_name:post_/powershoutcurrency/booking/delete,service:mobile-api-service,source:watchdog,story_category:apm,story_key:6187e3d6-b9cd-50b0-a17c-dd501115a16d,story_type:error_rate,team:ge-integration",info
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/424b3a073d98a4e2d433c6ad0fe4d437d7694424.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758677488000&to_ts=1758678688000&event_id=8294917036538356943&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 01:41:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758677488000&to_ts=1758678688000&event_id=8294917036538356943&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758678088000&to_ts=1758678388000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:46:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 01:41:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/18165dff4b1aded8c914e6a8cc39186f1b01be4f.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758677188000&to_ts=1758678388000&event_id=8294911997968413659&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 01:41:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758677188000&to_ts=1758678388000&event_id=8294911997968413659&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758677788000&to_ts=1758678088000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:41:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:39:50,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:39:50,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:39:50,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:39:50,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:39:50,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 01:39:50,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
"Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 01:39:50,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
Error rate increased on the DELETE /v1/exp/customers-loyaltyaccounts/?/bookings resource in loyaltyaccounts-exp-api,%%% New Watchdog Story detected on service loyaltyaccounts-exp-api  [View Watchdog Story](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  [![Metric Graph]()](/watchdog/story/3da5299b-8a43-56ed-8914-d2938f6c051a)  %%%,2025-09-24 01:36:49,"env:prod,resource_hash:5983621379bcf044,resource_name:delete_/v1/exp/customers-loyaltyaccounts/_/bookings,service:loyaltyaccounts-exp-api,source:watchdog,story_category:apm,story_key:3da5299b-8a43-56ed-8914-d2938f6c051a,story_type:error_rate,team:ge-integration",info
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/d930d25871b788ec7ecec1437412f2aa0fce79c3.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758676828000&to_ts=1758678028000&event_id=8294905956948862376&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 00:41:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758676828000&to_ts=1758678028000&event_id=8294905956948862376&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758677428000&to_ts=1758677728000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:35:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/7f4517938bf7e84868bb194b1431c292cd16f2b6.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758676799000&to_ts=1758677999000&event_id=8294906531490895701&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:18:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758676799000&to_ts=1758677999000&event_id=8294906531490895701&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758663299000&to_ts=1758677699000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:34:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/productandcontract/getsupplyagreement/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/870a08c7e87e2d25d9357e29aa5a7da5002ac6de.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fgetsupplyagreement%2Fv1&from_ts=1758676739000&to_ts=1758677939000&event_id=8294905532963674057&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:49:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fgetsupplyagreement%2Fv1&from_ts=1758676739000&to_ts=1758677939000&event_id=8294905532963674057&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fgetsupplyagreement%2Fv1&from_ts=1758663239000&to_ts=1758677639000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:33:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/productandcontract/getsupplyagreement/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/financial/accountevents/transactions}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/98d994aaa5e50ec2af9005757c1166f540a790c4.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758676679000&to_ts=1758677879000&event_id=8294904524695951463&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:19:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758676679000&to_ts=1758677879000&event_id=8294904524695951463&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758663179000&to_ts=1758677579000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:32:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/financial/accountevents/transactions,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P5] [Recovered] CPU is at 100% for Mule Host,"%%%   **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/5b6779e8f18fb7058066390c489670affa63bb9d.png)](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758676652000&to_ts=1758677852000&event_id=8294902995920229334&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:prt-api-p01,service:mule** was **> 0.0** at least once during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 01:19:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758676652000&to_ts=1758677852000&event_id=8294902995920229334&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View prt-api-p01](/infrastructure?filter=prt-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758677252000&to_ts=1758677672000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aprt-api-p01&from_ts=1758677252000&to_ts=1758677552000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:32:32,"env:prod,host:prt-api-p01,location:prt_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",success
[P3] [Recovered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/df865c743777e7b965523fd46f5599debe681c17.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758676622000&to_ts=1758677822000&event_id=8294902494580463535&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **<= 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:45:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758676622000&to_ts=1758677822000&event_id=8294902494580463535&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758676622000&to_ts=1758677642000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758676622000&to_ts=1758677522000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:32:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:30:09,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:30:09,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:30:09,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:30:09,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:30:09,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [Tier 1] Mule Prod API is reporting more errors than expected,%%%    @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/0934fcc2f5b502a10695d37032eee39710db3f76.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758676262000&to_ts=1758677462000&event_id=8294896460084431790&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:28:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758676262000&to_ts=1758677462000&event_id=8294896460084431790&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758676262000&to_ts=1758677162000&live=false&link_source=monitor_notif)] %%%,2025-09-24 01:26:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/expr/billingaccounts/v1/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/09d2fa4567d42c9dd77725a677e26b3ff5edf9d4.png)](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758676079000&to_ts=1758677279000&event_id=8294894467365455105&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:17:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758676079000&to_ts=1758677279000&event_id=8294894467365455105&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758662579000&to_ts=1758676979000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:22:59,"env:prod,monitor,priority:p3,resource_name:get_/expr/billingaccounts/v1/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:20:34,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:20:34,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:20:34,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:20:34,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:20:34,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
[P3] [Recovered on {resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/f7f299f034c02c9671379c0e69d6865ed745e5b8.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758675899000&to_ts=1758677099000&event_id=8294891498893966322&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:34:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758675899000&to_ts=1758677099000&event_id=8294891498893966322&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758662399000&to_ts=1758676799000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:19:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/pricing/getsupplyagreementpricing/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/8246e825057784bc05a3d79fb36a4e92dc509584.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758675899000&to_ts=1758677099000&event_id=8294891499482281086&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758675899000&to_ts=1758677099000&event_id=8294891499482281086&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758662399000&to_ts=1758676799000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:19:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/pricing/getsupplyagreementpricing/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P5] [Triggered] CPU is at 100% for Mule Host,"%%% Mule Host prt-api-p01 is reporting 100% CPU  **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/97af8ed5d0f43eb0b62a6877e2abeb52fa07c8a2.png)](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758675872000&to_ts=1758677072000&event_id=8294889908821375730&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:prt-api-p01,service:mule** was **<= 0.0** at all times during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 01:19:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758675872000&to_ts=1758677072000&event_id=8294889908821375730&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View prt-api-p01](/infrastructure?filter=prt-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758676472000&to_ts=1758676892000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aprt-api-p01&from_ts=1758676472000&to_ts=1758676772000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:19:32,"env:prod,host:prt-api-p01,location:prt_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",error
[P3] [Triggered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX error rate is at  1.0% over the last 5 minutes. PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/22f14275994cc391fe97ce3cafd1e55820911d2e.png)](/monitors/204752676?from_ts=1758675756000&to_ts=1758676956000&event_id=8294888972593549470&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 01:17:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758675756000&to_ts=1758676956000&event_id=8294888972593549470&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758662256000&to_ts=1758676656000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:17:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/cic/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/06646244b52118c6f7d2ad6d13d553c76130c4ae.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758675539000&to_ts=1758676739000&event_id=8294885422798862192&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758675539000&to_ts=1758676739000&event_id=8294885422798862192&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758662039000&to_ts=1758676439000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:13:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/70c8a4dee7af9804652d4ee3b91220473e7135d6.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758675539000&to_ts=1758676739000&event_id=8294885422660482348&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758675539000&to_ts=1758676739000&event_id=8294885422660482348&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758662039000&to_ts=1758676439000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:13:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P5] [Recovered] CPU is at 100% for Mule Host,"%%%   **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/84ed20e82b63f1a8e730d8a235965cf119c7ba97.png)](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758675452000&to_ts=1758676652000&event_id=8294882865088631549&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:prt-api-p01,service:mule** was **> 0.0** at least once during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 01:04:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758675452000&to_ts=1758676652000&event_id=8294882865088631549&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View prt-api-p01](/infrastructure?filter=prt-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758676052000&to_ts=1758676472000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aprt-api-p01&from_ts=1758676052000&to_ts=1758676352000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:12:32,"env:prod,host:prt-api-p01,location:prt_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",success
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:10:37,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:10:37,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:10:37,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:10:37,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:10:37,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
[P5] [Triggered] CPU is at 100% for Mule Host,"%%% Mule Host prt-api-p01 is reporting 100% CPU  **Links  *Host page : [](https://app.datadoghq.com/infrastructure?tags=service%3Amule%2Cenv%3Aprod) *Dashboard: [](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tile_focus=8088110735120782&tpl_var_host%5B0%5D=%2A&from_ts=1757976632053&to_ts=1757980232053&live=true)  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/090f57079893f971caae8e7db8e99c3af8f00e70.png)](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758674972000&to_ts=1758676172000&event_id=8294874818003397280&link_source=monitor_notif)  **system.cpu.idle** over **env:prod,host:prt-api-p01,service:mule** was **<= 0.0** at all times during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 01:04:32 UTC.  - - -  [[Monitor Status](/monitors/215054192?group=host%3Aprt-api-p01&from_ts=1758674972000&to_ts=1758676172000&event_id=8294874818003397280&link_source=monitor_notif)] · [[Edit Monitor](/monitors/215054192/edit?link_source=monitor_notif)] · [[View prt-api-p01](/infrastructure?filter=prt-api-p01&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758675572000&to_ts=1758675992000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-api-p01&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule%2C+env%3Aprod%29+AND+host%3Aprt-api-p01&from_ts=1758675572000&to_ts=1758675872000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:04:32,"env:prod,host:prt-api-p01,location:prt_datacentre,monitor,priority:p5,service:mule,source:alert,team:ge-integration",error
[P3] [Recovered on {resource_name:get_/cic/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/56371da5f23b130d5d199a0094a9cabd4f7bdb3d.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758674879000&to_ts=1758676079000&event_id=8294874324946659847&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758674879000&to_ts=1758676079000&event_id=8294874324946659847&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758661379000&to_ts=1758675779000&live=false&link_source=monitor_notif)] %%%",2025-09-24 01:02:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:00:22,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:00:22,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:00:22,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:00:22,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 01:00:22,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:50:47,"env:prod,resource_hash:704d1ffddcd69ecd,resource_name:get_/drd/initialize,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:50:47,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:50:47,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:50:47,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:50:47,"env:prod,resource_hash:329bf45034c1a5dc,resource_name:get_/usage/usageataglance,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
[P3] [Recovered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/03028edd03116ea12d880b3732bb56482479b744.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758673919000&to_ts=1758675119000&event_id=8294858232528445265&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758673919000&to_ts=1758675119000&event_id=8294858232528445265&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758660419000&to_ts=1758674819000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:46:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/financial/getlastbill/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/44cfacb740a1138919c34ddf5f1835bfe773a93b.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758673859000&to_ts=1758675059000&event_id=8294857214452315164&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758673859000&to_ts=1758675059000&event_id=8294857214452315164&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758660359000&to_ts=1758674759000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:45:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/financial/getlastbill/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/financial/getlastpayment/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/71b0f1950c933d95519164cfb1b27199d78dcc7d.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastpayment%2Fv1&from_ts=1758673859000&to_ts=1758675059000&event_id=8294857214302403649&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastpayment%2Fv1&from_ts=1758673859000&to_ts=1758675059000&event_id=8294857214302403649&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastpayment%2Fv1&from_ts=1758660359000&to_ts=1758674759000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:45:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/financial/getlastpayment/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
Latency increased on the GET /v1/exp/customers-billingaccounts/* resource in mule,%%%   [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/2ca40e7cc79896bfc3da6053777c655289b15674.png)](/watchdog/story/8ae5e015-383a-5b99-ba65-97fde34de1e9)  %%%,2025-09-24 00:45:47,"env:prod,service:mule,source:watchdog,story_category:usm,story_key:8ae5e015-383a-5b99-ba65-97fde34de1e9,story_type:latency,team:ge-integration",info
[P3] [Triggered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%% High 5-minute load average detected on host prt-mule-p02.genesispower.co.nz.  This alert means the system has more processes waiting to run than it can efficiently handle. A high load average often indicates that the host is under significant resource pressure — typically due to CPU saturation, blocked I/O, or memory contention. While some load is normal, consistently high load (especially relative to the number of CPU cores) can signal an unhealthy or overloaded system.  --- ## Impact  If load remains high, the host may experience:  - Slower system and application performance - Backlogged scheduled jobs or delayed processing - Increased risk of timeouts, dropped requests, or service degradation.   High load can quickly lead to cascading failures or degraded user experience for upstream services EIQ and frontend and downstream services including Gentrack.   ### Initial troubleshooting  1. Identify the affected host from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for prt-mule-p02.genesispower.co.nz. 3. In the **Metrics tab**, examine:    - `system.load.1`    - `system.cpu.user`, `system.cpu.system`, `system.io.await`, `system.mem.pct_usable` 4. Use [**Live Processes**](https://app.datadoghq.com/infrastructure/processes) to find CPU- or I/O-heavy processes.              ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3Aprt-mule-p02.genesispower.co.nz) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/b15d50ee3b0f0166f1a6ed76ab3346247473430a.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758673802000&to_ts=1758675002000&event_id=8294855179642131017&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **> 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:45:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758673802000&to_ts=1758675002000&event_id=8294855179642131017&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758673802000&to_ts=1758674822000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758673802000&to_ts=1758674702000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:45:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [Tier 1] Mule Prod API is reporting more errors than expected,%%% Mule Prod  API get_/v1/exp/supplypointandmetering/devices is reporting higher  error rate than expected     @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/57bb08b770a58150613d283ee0dad97e202bb38e.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758673742000&to_ts=1758674942000&event_id=8294854188714888298&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:44:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758673742000&to_ts=1758674942000&event_id=8294854188714888298&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758673742000&to_ts=1758674642000&live=false&link_source=monitor_notif)] %%%,2025-09-24 00:44:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [Tier 1] Mule Prod API is reporting more errors than expected,%%%    @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/cc717ec7b785c77bf37ad464cabaf1c0e08de469.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758673622000&to_ts=1758674822000&event_id=8294852163976653137&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758673622000&to_ts=1758674822000&event_id=8294852163976653137&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758673622000&to_ts=1758674522000&live=false&link_source=monitor_notif)] %%%,2025-09-24 00:42:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-24 00:41:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/af4a8879bafa3caf4175ec887dce4ab3ff153eac.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758673588000&to_ts=1758674788000&event_id=8294851593770565716&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Wed Sep 24 2025 00:41:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758673588000&to_ts=1758674788000&event_id=8294851593770565716&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758674188000&to_ts=1758674488000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:41:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  The monitor was marked as **Recovered** on **env:prod,http.status_code:500,service:esb-connector-service** by jacqui.rennie@ddsit.co.nz.  The monitor was last triggered at Tue Sep 23 2025 23:55:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758673540000&to_ts=1758674740000&event_id=8294849787278840973&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758674140000&to_ts=1758674440000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:40:40,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting org.mule.module.reboot.mulecontainerbootstrap",%%% New Watchdog Story detected on service org.mule.module.reboot.mulecontainerbootstrap  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:40:32,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:org.mule.module.reboot.mulecontainerbootstrap,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
"Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:40:32,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:40:32,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
[P3] [Recovered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/77f2bd79282aecd2a2b48a7453f694ea6e684877.png)](/monitors/204752676?from_ts=1758673476000&to_ts=1758674676000&event_id=8294850730007224024&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 00:20:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758673476000&to_ts=1758674676000&event_id=8294850730007224024&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758659976000&to_ts=1758674376000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:39:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/customer/getaccountbalance/v2}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/9987c74277aaddd68cca86d457494eda9564d622.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetaccountbalance%2Fv2&from_ts=1758673439000&to_ts=1758674639000&event_id=8294850187082172262&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetaccountbalance%2Fv2&from_ts=1758673439000&to_ts=1758674639000&event_id=8294850187082172262&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetaccountbalance%2Fv2&from_ts=1758659939000&to_ts=1758674339000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:38:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/customer/getaccountbalance/v2,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:post_/esb/pricing/getsupplyagreementpricing/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/pricing/getsupplyagreementpricing/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/ca171356d752905ea8aa6e54accab7cda57c67b9.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758673379000&to_ts=1758674579000&event_id=8294849176601172388&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758673379000&to_ts=1758674579000&event_id=8294849176601172388&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758659879000&to_ts=1758674279000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:37:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/pricing/getsupplyagreementpricing/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/productandcontract/supplyagreements/_/pricing.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/343f9c165c4497dc7390976474ef26a6607769c2.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758673199000&to_ts=1758674399000&event_id=8294846150031422489&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:34:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758673199000&to_ts=1758674399000&event_id=8294846150031422489&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758659699000&to_ts=1758674099000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:34:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/204bd63d-febb-537c-9fb1-f47facf8304a)  [![Metric Graph]()](/watchdog/story/204bd63d-febb-537c-9fb1-f47facf8304a)  %%%,2025-09-24 00:30:16,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:204bd63d-febb-537c-9fb1-f47facf8304a,story_type:latency,team:ge-integration",info
"Latency increased on 3 different resources in esb-datasource-service, impacting mobile-api-service",%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  [![Metric Graph]()](/watchdog/story/1672dbc7-51f4-5c06-9901-44511c60eda5)  %%%,2025-09-24 00:30:16,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:1672dbc7-51f4-5c06-9901-44511c60eda5,story_type:latency,team:ge-integration",info
[P3] [Recovered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/7fdebf450defdfcc2bb1bd612743c2b5558426b3.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758672902000&to_ts=1758674102000&event_id=8294840088385910656&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **<= 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:18:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758672902000&to_ts=1758674102000&event_id=8294840088385910656&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758672902000&to_ts=1758673922000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758672902000&to_ts=1758673802000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:30:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
Latency increased on the GET /v1/exp/customers-billingaccounts/? resource in billingaccounts-exp-api,%%% New Watchdog Story detected on service billingaccounts-exp-api  [View Watchdog Story](/watchdog/story/204bd63d-febb-537c-9fb1-f47facf8304a)  [![Metric Graph]()](/watchdog/story/204bd63d-febb-537c-9fb1-f47facf8304a)  %%%,2025-09-24 00:28:26,"env:prod,resource_hash:c224ee4a5027ef29,resource_name:get_/v1/exp/customers-billingaccounts/,service:billingaccounts-exp-api,source:watchdog,story_category:apm,story_key:204bd63d-febb-537c-9fb1-f47facf8304a,story_type:latency,team:ge-integration",info
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [Tier 1] Mule Prod API is reporting more errors than expected,%%% Mule Prod  API get_/v1/exp/supplypointandmetering/consumption is reporting higher  error rate than expected     @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/7a5e7e45f76f1883f764a76daaada2e85638ca14.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758672782000&to_ts=1758673982000&event_id=8294838160331809993&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:28:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758672782000&to_ts=1758673982000&event_id=8294838160331809993&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758672782000&to_ts=1758673682000&live=false&link_source=monitor_notif)] %%%,2025-09-24 00:28:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:post_/esb/customer/getbillingaccount/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/43f2c068264f10493bb7f4f817a09f1fd07f09fa.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758672599000&to_ts=1758673799000&event_id=8294836072596979353&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758672599000&to_ts=1758673799000&event_id=8294836072596979353&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758659099000&to_ts=1758673499000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:24:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/customer/getbillingaccount/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 1 ] Error rate  on Prod esb-connector-service is increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/b62a4e882b98593310a7584cccdcb87f8a08b7af.png)](/monitors/211776123?group=http.status_code%3A500&from_ts=1758672423000&to_ts=1758673623000&event_id=8294832043991348544&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 500.0** in total during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:19:03 UTC.  - - -  [[Monitor Status](/monitors/211776123?group=http.status_code%3A500&from_ts=1758672423000&to_ts=1758673623000&event_id=8294832043991348544&link_source=monitor_notif)] · [[Edit Monitor](/monitors/211776123/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service+%2Cenv%3Aprod%29+AND+http.status_code%3A500&from_ts=1758672423000&to_ts=1758673323000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:22:03,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX error rate is at  1.0% over the last 5 minutes. PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/641ef1c06bbcf34bc4a61577328780d8edbcc3cf.png)](/monitors/204752676?from_ts=1758672336000&to_ts=1758673536000&event_id=8294831595619425211&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Wed Sep 24 2025 00:20:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758672336000&to_ts=1758673536000&event_id=8294831595619425211&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758658836000&to_ts=1758673236000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:20:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
Error rate increased on the GET /drd/navigation resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/cfe1c5dc-f46e-5bb5-b219-c4ab05bedc08)  [![Metric Graph]()](/watchdog/story/cfe1c5dc-f46e-5bb5-b219-c4ab05bedc08)  %%%,2025-09-24 00:20:30,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:cfe1c5dc-f46e-5bb5-b219-c4ab05bedc08,story_type:error_rate,team:ge-integration",info
[P3] [Triggered on {resource_name:patch_/v1/exp/customers/payments/result/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request patch_/v1/exp/customers/payments/result/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/e333ad9a55af5d8577326d6cac002e1e7057a3bb.png)](/monitors/213661559?group=resource_name%3Apatch_%2Fv1%2Fexp%2Fcustomers%2Fpayments%2Fresult%2F&from_ts=1758672299000&to_ts=1758673499000&event_id=8294831040590369679&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:19:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apatch_%2Fv1%2Fexp%2Fcustomers%2Fpayments%2Fresult%2F&from_ts=1758672299000&to_ts=1758673499000&event_id=8294831040590369679&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apatch_%2Fv1%2Fexp%2Fcustomers%2Fpayments%2Fresult%2F&from_ts=1758658799000&to_ts=1758673199000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:19:59,"env:prod,monitor,priority:p3,resource_name:patch_/v1/exp/customers/payments/result/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/financial/accountevents/transactions}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/financial/accountevents/transactions.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/6706bb0b49b00695174a23cfc4f47946c7644536.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758672299000&to_ts=1758673499000&event_id=8294831040487066456&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:19:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758672299000&to_ts=1758673499000&event_id=8294831040487066456&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Ffinancial%2Faccountevents%2Ftransactions&from_ts=1758658799000&to_ts=1758673199000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:19:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/financial/accountevents/transactions,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {http.status_code:500}] [Tier 1 ] Error rate  on Prod esb-connector-service is increasing,"%%% 500 error rate on prod esb-connector-service has increased  2025-09-24 00:19:03 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   - Upstream impact to other  EIQ services  - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/42507ca3239e62468b28cdd142c4b12f3263067d.png)](/monitors/211776123?group=http.status_code%3A500&from_ts=1758672243000&to_ts=1758673443000&event_id=8294829034632303919&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 500.0** in total during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:19:03 UTC.  - - -  [[Monitor Status](/monitors/211776123?group=http.status_code%3A500&from_ts=1758672243000&to_ts=1758673443000&event_id=8294829034632303919&link_source=monitor_notif)] · [[Edit Monitor](/monitors/211776123/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service+%2Cenv%3Aprod%29+AND+http.status_code%3A500&from_ts=1758672243000&to_ts=1758673143000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:19:03,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/devices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/supplypointandmetering/devices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/6ae2bea6906f7a96ef2dfec243c99d0c5693cc47.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758672239000&to_ts=1758673439000&event_id=8294830091259682280&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:18:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758672239000&to_ts=1758673439000&event_id=8294830091259682280&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fdevices&from_ts=1758658739000&to_ts=1758673139000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:18:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/devices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%% High 5-minute load average detected on host prt-mule-p02.genesispower.co.nz.  This alert means the system has more processes waiting to run than it can efficiently handle. A high load average often indicates that the host is under significant resource pressure — typically due to CPU saturation, blocked I/O, or memory contention. While some load is normal, consistently high load (especially relative to the number of CPU cores) can signal an unhealthy or overloaded system.  --- ## Impact  If load remains high, the host may experience:  - Slower system and application performance - Backlogged scheduled jobs or delayed processing - Increased risk of timeouts, dropped requests, or service degradation.   High load can quickly lead to cascading failures or degraded user experience for upstream services EIQ and frontend and downstream services including Gentrack.   ### Initial troubleshooting  1. Identify the affected host from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for prt-mule-p02.genesispower.co.nz. 3. In the **Metrics tab**, examine:    - `system.load.1`    - `system.cpu.user`, `system.cpu.system`, `system.io.await`, `system.mem.pct_usable` 4. Use [**Live Processes**](https://app.datadoghq.com/infrastructure/processes) to find CPU- or I/O-heavy processes.              ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3Aprt-mule-p02.genesispower.co.nz) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/cf8b6427b91f1ed9c7da5d963a31801f599815da.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758672182000&to_ts=1758673382000&event_id=8294828011039514093&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **> 350.0** on average during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:18:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758672182000&to_ts=1758673382000&event_id=8294828011039514093&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758672182000&to_ts=1758673202000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758672182000&to_ts=1758673082000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:18:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/expr/billingaccounts/v1/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/expr/billingaccounts/v1/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/6d71cd55004a5e17cc249e91946d6b46ff01c512.png)](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758672179000&to_ts=1758673379000&event_id=8294829021342318364&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Wed Sep 24 2025 00:17:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758672179000&to_ts=1758673379000&event_id=8294829021342318364&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fexpr%2Fbillingaccounts%2Fv1%2F&from_ts=1758658679000&to_ts=1758673079000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:17:59,"env:prod,monitor,priority:p3,resource_name:get_/expr/billingaccounts/v1/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
Error rate increased on the GET /drd/navigation resource in mobile-api-service,%%% New Watchdog Story detected on service mobile-api-service  [View Watchdog Story](/watchdog/story/cfe1c5dc-f46e-5bb5-b219-c4ab05bedc08)  [![Metric Graph]()](/watchdog/story/cfe1c5dc-f46e-5bb5-b219-c4ab05bedc08)  %%%,2025-09-24 00:17:47,"env:prod,resource_hash:84f3f138dc818fb8,resource_name:get_/drd/navigation,service:mobile-api-service,source:watchdog,story_category:apm,story_key:cfe1c5dc-f46e-5bb5-b219-c4ab05bedc08,story_type:error_rate,team:ge-integration",info
[P3] [Recovered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-24/5e0c259ded052821d53cf174556d778d271dc2e0.png)](/monitors/204752676?from_ts=1758671436000&to_ts=1758672636000&event_id=8294816505568811486&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Tue Sep 23 2025 23:33:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758671436000&to_ts=1758672636000&event_id=8294816505568811486&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758657936000&to_ts=1758672336000&live=false&link_source=monitor_notif)] %%%",2025-09-24 00:05:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-23 23:55:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/b8b5a97fac30a0b36dc630965f3978e4924f58ca.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670828000&to_ts=1758672028000&event_id=8294805288266180575&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 23:55:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670828000&to_ts=1758672028000&event_id=8294805288266180575&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758671428000&to_ts=1758671728000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:55:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/4531739ee09049dd6cf1aa938936e15cc5a16bca.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670708000&to_ts=1758671908000&event_id=8294803276321413191&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 23:48:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670708000&to_ts=1758671908000&event_id=8294803276321413191&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758671308000&to_ts=1758671608000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:53:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-23 23:48:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/961bed6c1c9ca183ebb21f1fd96b218c769f07c7.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670408000&to_ts=1758671608000&event_id=8294798249605134418&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 23:48:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670408000&to_ts=1758671608000&event_id=8294798249605134418&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758671008000&to_ts=1758671308000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:48:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/90baa3600e3dea86562f78f9f9857e4f661b4e34.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670168000&to_ts=1758671368000&event_id=8294794213740368332&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 23:24:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758670168000&to_ts=1758671368000&event_id=8294794213740368332&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758670768000&to_ts=1758671068000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:44:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P2] [Recovered] [Tier 1] Mule Error rate for API requests has exceeded 10 errors per second,"%%%   @slack-integration-support @teams-monitoring  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/b0a6a65645aad8c5a811a2a4edab6fe498fc3003.png)](/monitors/213663855?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758670035000&to_ts=1758671235000&event_id=8294791992090590242&link_source=monitor_notif)  The change in **trace.grizzly.request.errors** over **env:prod,resource_name:get_/v1/exp/customers-billingaccounts/** for a **15m** avg was **<= 3.0** compared to **5m ago**.  The monitor was last triggered at Tue Sep 23 2025 23:27:15 UTC.  - - -  [[Monitor Status](/monitors/213663855?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758670035000&to_ts=1758671235000&event_id=8294791992090590242&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213663855/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758670635000&to_ts=1758670935000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:42:15,"env:prod,monitor,priority:p2,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/cic/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/cic/v1/exp/customers-billingaccounts.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/54f2423b8d4fa0d662c67160c013490abf9729a5.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789772731348842&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789772731348842&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758656339000&to_ts=1758670739000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:38:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-billingaccounts.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/0b6f63780e641a26c9bde7c09aedd18b8a214390.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789772721402796&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789772721402796&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758656339000&to_ts=1758670739000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:38:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/339b10a38c24b080b9f2f78b046af4bcd0695be1.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fvouchers&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789773649188555&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:52:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fvouchers&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789773649188555&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fvouchers&from_ts=1758656339000&to_ts=1758670739000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:38:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-billingaccounts/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/65916eeb785e901a8b01e7779f727c75a1fa3143.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789773382089228&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789773382089228&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758656339000&to_ts=1758670739000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:38:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/cic/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/cic/v1/exp/customers-billingaccounts/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/268e000d825957ad2c74d146cec15c54849b4cba.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789777076058934&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669839000&to_ts=1758671039000&event_id=8294789777076058934&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758656339000&to_ts=1758670739000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:38:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/customer/getaccountbalance/v2}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/customer/getaccountbalance/v2.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/ff6783c3c310a2eb75b6526673fed4bfe437fcf3.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetaccountbalance%2Fv2&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790678388959&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetaccountbalance%2Fv2&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790678388959&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetaccountbalance%2Fv2&from_ts=1758656279000&to_ts=1758670679000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:37:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/customer/getaccountbalance/v2,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/financial/getlastpayment/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/financial/getlastpayment/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/acf8919a93b01ee874e23089866f138ad66c7e61.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastpayment%2Fv1&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790534856174&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastpayment%2Fv1&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790534856174&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastpayment%2Fv1&from_ts=1758656279000&to_ts=1758670679000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:37:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/financial/getlastpayment/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/customer/getbillingaccount/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/customer/getbillingaccount/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/831346b656180b698b6d3ac9ab1a8e1df17303c8.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790972250417&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790972250417&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758656279000&to_ts=1758670679000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:37:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/customer/getbillingaccount/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/financial/getlastbill/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/financial/getlastbill/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/f5f3db3cbca404df8771000409a14c14d4d8c988.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790219616134&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758669779000&to_ts=1758670979000&event_id=8294788790219616134&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758656279000&to_ts=1758670679000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:37:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/financial/getlastbill/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [Tier 1] Mule Prod API is reporting more errors than expected,%%% Mule Prod  API get_/v1/exp/customers-billingaccounts/ is reporting higher  error rate than expected     @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/4f975ccc254ea9b556ffa76c5f75baa63b2ef204.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669722000&to_ts=1758670922000&event_id=8294786736972782241&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:37:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669722000&to_ts=1758670922000&event_id=8294786736972782241&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669722000&to_ts=1758670622000&live=false&link_source=monitor_notif)] %%%,2025-09-23 23:37:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX error rate is at  1.0% over the last 5 minutes. PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/9912b74288a566cf0be46e79280b264e5efb2c2c.png)](/monitors/204752676?from_ts=1758669516000&to_ts=1758670716000&event_id=8294784282877095379&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Tue Sep 23 2025 23:33:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758669516000&to_ts=1758670716000&event_id=8294784282877095379&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758656016000&to_ts=1758670416000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:33:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P2] [Triggered] [Tier 1] Mule Error rate for API requests has exceeded 10 errors per second,"%%% Mule error rate for API get_/v1/exp/customers-billingaccounts/requests has exceeded 10 errors per second   @slack-integration-support @teams-monitoring  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/99aa3f83e2a606e090d4af7abb63c095cc8d720d.png)](/monitors/213663855?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669135000&to_ts=1758670335000&event_id=8294776888606518210&link_source=monitor_notif)  The change in **trace.grizzly.request.errors** over **env:prod,resource_name:get_/v1/exp/customers-billingaccounts/** for a **15m** avg was **> 3.0** compared to **5m ago**.  The monitor was last triggered at Tue Sep 23 2025 23:27:15 UTC.  - - -  [[Monitor Status](/monitors/213663855?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669135000&to_ts=1758670335000&event_id=8294776888606518210&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213663855/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758669735000&to_ts=1758670035000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:27:15,"env:prod,monitor,priority:p2,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/1ad441fcafb76f96bad14abe0ce5029f463f0587.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758668999000&to_ts=1758670199000&event_id=8294775692167607135&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758668999000&to_ts=1758670199000&event_id=8294775692167607135&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758655499000&to_ts=1758669899000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:24:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-23 23:24:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/2e92b37672cfa336d8e279b2a01a969a01789f71.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758668968000&to_ts=1758670168000&event_id=8294774091487835864&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 23:24:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758668968000&to_ts=1758670168000&event_id=8294774091487835864&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758669568000&to_ts=1758669868000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:24:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/7c2f1fdd0548b4ca1c35ef4affd58cf658efc4c4.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758668402000&to_ts=1758669602000&event_id=8294764583284057953&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **<= 350.0** on average during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:03:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758668402000&to_ts=1758669602000&event_id=8294764583284057953&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758668402000&to_ts=1758669422000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758668402000&to_ts=1758669302000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:15:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/ee1f9c1fd3795fcdda0f62779a648e0ed0954a36.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758668068000&to_ts=1758669268000&event_id=8294758981564479817&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 22:49:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758668068000&to_ts=1758669268000&event_id=8294758981564479817&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758668668000&to_ts=1758668968000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:09:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {host:prt-mule-p02.genesispower.co.nz}] [Tier 1] Mule Prod - System load is high for host,"%%% High 5-minute load average detected on host prt-mule-p02.genesispower.co.nz.  This alert means the system has more processes waiting to run than it can efficiently handle. A high load average often indicates that the host is under significant resource pressure — typically due to CPU saturation, blocked I/O, or memory contention. While some load is normal, consistently high load (especially relative to the number of CPU cores) can signal an unhealthy or overloaded system.  --- ## Impact  If load remains high, the host may experience:  - Slower system and application performance - Backlogged scheduled jobs or delayed processing - Increased risk of timeouts, dropped requests, or service degradation.   High load can quickly lead to cascading failures or degraded user experience for upstream services EIQ and frontend and downstream services including Gentrack.   ### Initial troubleshooting  1. Identify the affected host from the alert. 2. Open [**Infrastructure > Host Details**](https://app.datadoghq.com/infrastructure/host) for prt-mule-p02.genesispower.co.nz. 3. In the **Metrics tab**, examine:    - `system.load.1`    - `system.cpu.user`, `system.cpu.system`, `system.io.await`, `system.mem.pct_usable` 4. Use [**Live Processes**](https://app.datadoghq.com/infrastructure/processes) to find CPU- or I/O-heavy processes.              ###  Related links  * [Host Map](https://app.datadoghq.com/infrastructure/map) * [Live Processes](https://app.datadoghq.com/process?query=host%3Aprt-mule-p02.genesispower.co.nz) * [System Dashboards](https://app.datadoghq.com/dashboard/lists) * [Monitor Documentation](https://docs.datadoghq.com/monitors/)  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/580dc682f0c4b62121a57b0b887bfec460f20742.png)](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758667682000&to_ts=1758668882000&event_id=8294752502264460108&link_source=monitor_notif)  **system.load.5** over **env:prod,host:prt-mule-p02.genesispower.co.nz,service:mule** was **> 350.0** on average during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 23:03:02 UTC.  - - -  [[Monitor Status](/monitors/204790322?group=host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758667682000&to_ts=1758668882000&event_id=8294752502264460108&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204790322/edit?link_source=monitor_notif)] · [[View prt-mule-p02.genesispower.co.nz](/infrastructure?filter=prt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Show Processes](/process?from_ts=1758667682000&to_ts=1758668702000&live=false&showSummaryGraphs=true&sort=cpu%2CDESC&query=host%3Aprt-mule-p02.genesispower.co.nz&link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Amule+%2C+env%3Aprod%29+AND+host%3Aprt-mule-p02.genesispower.co.nz&from_ts=1758667682000&to_ts=1758668582000&live=false&link_source=monitor_notif)] %%%",2025-09-23 23:03:02,"env:prod,host:prt-mule-p02.genesispower.co.nz,location:prt_datacentre,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-23 22:49:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/5e9806c523221734f9e705a873d7400a442e56e4.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758666868000&to_ts=1758668068000&event_id=8294738849438185997&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 22:49:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758666868000&to_ts=1758668068000&event_id=8294738849438185997&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758667468000&to_ts=1758667768000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:49:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/944d735e9527e5665f78ffeecea0564a2f31ca3f.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758666839000&to_ts=1758668039000&event_id=8294739433197587778&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:13:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758666839000&to_ts=1758668039000&event_id=8294739433197587778&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758653339000&to_ts=1758667739000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:48:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/cic/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/e0f9c8660966824185f1d0e0a5e669ee3f1cd513.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758666839000&to_ts=1758668039000&event_id=8294739433394929356&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:13:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758666839000&to_ts=1758668039000&event_id=8294739433394929356&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758653339000&to_ts=1758667739000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:48:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/esb/finance/loyaltyvouchers/v1/expiring-vouchers}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/92d1806401cd239e9faa11b6c779696b35f7854d.png)](/monitors/213661559?group=resource_name%3Aget_%2Fesb%2Ffinance%2Floyaltyvouchers%2Fv1%2Fexpiring-vouchers&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734401359822270&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:50:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fesb%2Ffinance%2Floyaltyvouchers%2Fv1%2Fexpiring-vouchers&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734401359822270&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fesb%2Ffinance%2Floyaltyvouchers%2Fv1%2Fexpiring-vouchers&from_ts=1758653039000&to_ts=1758667439000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:43:59,"env:prod,monitor,priority:p3,resource_name:get_/esb/finance/loyaltyvouchers/v1/expiring-vouchers,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/0f28c34be0d3e6a71b83fba05db1cefe048b9dbe.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734404094508480&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:11:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734404094508480&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758653039000&to_ts=1758667439000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:43:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/cb664b1d7fd143088cc21d31fc0504ddec360a52.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734402553146209&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734402553146209&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758653039000&to_ts=1758667439000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:43:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/09cac5dd08fa55d239b1861db7ca4e954ca9a33e.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbalance%2Fexpiring-vouchers&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734402470662646&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:50:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbalance%2Fexpiring-vouchers&from_ts=1758666539000&to_ts=1758667739000&event_id=8294734402470662646&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbalance%2Fexpiring-vouchers&from_ts=1758653039000&to_ts=1758667439000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:43:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/df32adcf4432d626a24262ea8e37566722e89810.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbookings&from_ts=1758666479000&to_ts=1758667679000&event_id=8294733390971785258&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:52:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbookings&from_ts=1758666479000&to_ts=1758667679000&event_id=8294733390971785258&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbookings&from_ts=1758652979000&to_ts=1758667379000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:42:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/cic/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/78c73fb6ba810f9ea5db5d50e95142d04423363a.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758666479000&to_ts=1758667679000&event_id=8294733391238425437&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:13:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758666479000&to_ts=1758667679000&event_id=8294733391238425437&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758652979000&to_ts=1758667379000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:42:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/v1/expr/financial/accountevent/invoices}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/expr/financial/accountevent/invoices.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/f29c34cbd2967da134c529042a9d6956c5d24771.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758666239000&to_ts=1758667439000&event_id=8294729373472294797&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:38:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758666239000&to_ts=1758667439000&event_id=8294729373472294797&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexpr%2Ffinancial%2Faccountevent%2Finvoices&from_ts=1758652739000&to_ts=1758667139000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:38:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/expr/financial/accountevent/invoices,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [Tier 1] Mule Prod API is reporting more errors than expected,%%%    @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/520e846a2b737e457154825c8d6d657a9a33c3c3.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758666122000&to_ts=1758667322000&event_id=8294726349917148701&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:11:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758666122000&to_ts=1758667322000&event_id=8294726349917148701&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758666122000&to_ts=1758667022000&live=false&link_source=monitor_notif)] %%%,2025-09-23 22:37:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/customer/getbillingaccount/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/2d535143d79d8aa90b8e97ff5649431ccdd74fde.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758666119000&to_ts=1758667319000&event_id=8294727390614380183&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758666119000&to_ts=1758667319000&event_id=8294727390614380183&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758652619000&to_ts=1758667019000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:36:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/customer/getbillingaccount/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/8317ac5cd4e76d3ec57c6c8305ebbabbb13baa5a.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758666088000&to_ts=1758667288000&event_id=8294725758959163434&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 22:21:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758666088000&to_ts=1758667288000&event_id=8294725758959163434&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758666688000&to_ts=1758666988000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:36:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Recovered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/861408eb158d6fdb1593c650161e681346c5882f.png)](/monitors/204752676?from_ts=1758665196000&to_ts=1758666396000&event_id=8294711803493123830&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Tue Sep 23 2025 22:09:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758665196000&to_ts=1758666396000&event_id=8294711803493123830&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758651696000&to_ts=1758666096000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:21:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-23 22:21:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/394a7d6425753b1d77f640cc301009f54b05af09.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758665188000&to_ts=1758666388000&event_id=8294710666242209016&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 22:21:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758665188000&to_ts=1758666388000&event_id=8294710666242209016&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758665788000&to_ts=1758666088000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:21:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/f08c3e8d31aa3c0f791a5edf45fb13943b3ca902.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758664948000&to_ts=1758666148000&event_id=8294706645546227274&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 22:01:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758664948000&to_ts=1758666148000&event_id=8294706645546227274&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758665548000&to_ts=1758665848000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:17:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {resource_name:get_/cic/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/cic/v1/exp/customers-billingaccounts/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/36fdd80648bfb683ee7a53d2fd878b47fce7bfc2.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758664739000&to_ts=1758665939000&event_id=8294704306612005057&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:13:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758664739000&to_ts=1758665939000&event_id=8294704306612005057&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758651239000&to_ts=1758665639000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:13:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-billingaccounts.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/d8a9b4448dc223f5dce965f16f4109756e782c1e.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758664739000&to_ts=1758665939000&event_id=8294704307005414599&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:13:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758664739000&to_ts=1758665939000&event_id=8294704307005414599&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758651239000&to_ts=1758665639000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:13:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/cic/v1/exp/customers-billingaccounts}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/cic/v1/exp/customers-billingaccounts.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/a14db15eab5b4366ed2025ac6364873af8de16fb.png)](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758664739000&to_ts=1758665939000&event_id=8294704305528052232&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:13:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758664739000&to_ts=1758665939000&event_id=8294704305528052232&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fcic%2Fv1%2Fexp%2Fcustomers-billingaccounts&from_ts=1758651239000&to_ts=1758665639000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:13:59,"env:prod,monitor,priority:p3,resource_name:get_/cic/v1/exp/customers-billingaccounts,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/supplypointandmetering/consumption}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/supplypointandmetering/consumption.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/9fcffcb9f95f06895e770e8ad8b3958115db390c.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758664619000&to_ts=1758665819000&event_id=8294702207337772514&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:11:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758664619000&to_ts=1758665819000&event_id=8294702207337772514&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fsupplypointandmetering%2Fconsumption&from_ts=1758651119000&to_ts=1758665519000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:11:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/supplypointandmetering/consumption,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [Tier 1] Mule Prod API is reporting more errors than expected,%%% Mule Prod  API get_/v1/exp/customers-billingaccounts/ is reporting higher  error rate than expected     @slack-integration-support @teams-monitoring  @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/459fed1bc1888180b131b79caf0914cf11c1d38d.png)](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758664562000&to_ts=1758665762000&event_id=8294700177629260435&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request.errors{env:prod}.as_rate()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:11:02 UTC.  - - -  [[Monitor Status](/monitors/213660002?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758664562000&to_ts=1758665762000&event_id=8294700177629260435&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213660002/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758664562000&to_ts=1758665462000&live=false&link_source=monitor_notif)] %%%,2025-09-23 22:11:02,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/customer/getbillingaccount/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/customer/getbillingaccount/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/6bfd2f4476bf7e9991910c42512e50a3749d68b8.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758664559000&to_ts=1758665759000&event_id=8294701191109905554&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758664559000&to_ts=1758665759000&event_id=8294701191109905554&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fcustomer%2Fgetbillingaccount%2Fv1&from_ts=1758651059000&to_ts=1758665459000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:10:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/customer/getbillingaccount/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-billingaccounts/}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-billingaccounts/.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/6dacc3fc0939950ef7887c697a1f90de84f43445.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758664559000&to_ts=1758665759000&event_id=8294701191559320843&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 22:10:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758664559000&to_ts=1758665759000&event_id=8294701191559320843&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-billingaccounts%2F&from_ts=1758651059000&to_ts=1758665459000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:10:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-billingaccounts/,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered] [Tier 1] PROD NGINX error rate has increased,"%%%  PROD NGINX error rate is at  1.0% over the last 5 minutes. PROD NGINX is reporting an elevated error rate of `1.0%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.      @slack-Genesis_Energy-integration-support   ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/e30a22af7dea479b6bcffcfaad088b7a733eea3e.png)](/monitors/204752676?from_ts=1758664476000&to_ts=1758665676000&event_id=8294699733200677693&link_source=monitor_notif)  At least **100%** of **`(sum:universal.http.server{env:prod,error:true,service:mule}.as_count() / sum:universal.http.server{env:prod,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Tue Sep 23 2025 22:09:36 UTC.  - - -  [[Monitor Status](/monitors/204752676?from_ts=1758664476000&to_ts=1758665676000&event_id=8294699733200677693&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204752676/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2Cenv%3Aprod%29+OR+%28service%3Amule+%2Cenv%3Aprod%29&from_ts=1758650976000&to_ts=1758665376000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:09:36,"env:prod,error:true,monitor,priority:p3,service:mule,source:alert,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-23 22:01:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/8ab793d95b505da7f9c050d94ce461364552972e.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758663988000&to_ts=1758665188000&event_id=8294690533555858709&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 22:01:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758663988000&to_ts=1758665188000&event_id=8294690533555858709&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758664588000&to_ts=1758664888000&live=false&link_source=monitor_notif)] %%%",2025-09-23 22:01:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Recovered on {resource_name:post_/get}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/1cdc4bb8ed0137ce46430f9aed3869a0dd5273ea.png)](/monitors/213661559?group=resource_name%3Apost_%2Fget&from_ts=1758663719000&to_ts=1758664919000&event_id=8294687106861734499&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:36:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fget&from_ts=1758663719000&to_ts=1758664919000&event_id=8294687106861734499&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fget&from_ts=1758650219000&to_ts=1758664619000&live=false&link_source=monitor_notif)] %%%",2025-09-23 21:56:59,"env:prod,monitor,priority:p3,resource_name:post_/get,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/financial/getlastbill/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/928f1acaafc11d9b5ab5fb3f008a586d25a72b5f.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758662639000&to_ts=1758663839000&event_id=8294668967622620249&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:50:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758662639000&to_ts=1758663839000&event_id=8294668967622620249&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758649139000&to_ts=1758663539000&live=false&link_source=monitor_notif)] %%%",2025-09-23 21:38:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/financial/getlastbill/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/1f064f343d57c2aaa488a0b46f8a4f7d859b6be6.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758661139000&to_ts=1758662339000&event_id=8294643810548361131&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:33:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758661139000&to_ts=1758662339000&event_id=8294643810548361131&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758647639000&to_ts=1758662039000&live=false&link_source=monitor_notif)] %%%",2025-09-23 21:13:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {resource_name:post_/esb/pricing/getsupplyagreementpricing/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%%  @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/14f82a097d489c48f407cdd0ad59fb770197893e.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758661139000&to_ts=1758662339000&event_id=8294643810548546324&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:33:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758661139000&to_ts=1758662339000&event_id=8294643810548546324&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758647639000&to_ts=1758662039000&live=false&link_source=monitor_notif)] %%%",2025-09-23 21:13:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/pricing/getsupplyagreementpricing/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",success
[P3] [Recovered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%%   @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/3420d9a3e731fdb79e69855db9fa141a3c433344.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758660808000&to_ts=1758662008000&event_id=8294637180452716318&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **<= 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 21:03:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758660808000&to_ts=1758662008000&event_id=8294637180452716318&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758661408000&to_ts=1758661708000&live=false&link_source=monitor_notif)] %%%",2025-09-23 21:08:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",success
[P3] [Triggered on {http.status_code:500}] [Tier 3 ] Errors on Prod  esb-connector-service are increasing,"%%% 50.0 errors on prod esb connector-service since 2025-09-23 21:03:28 This can have a downstream and upstream effect on EIQ services and other endpoints  **  Impact   If Error rate is high   - Upstream impact to EIQ   - increase in 500 errors for customers  - Downstream systems may also be affected  ** Troubleshooting - Check which api's are reporting errors - Assign ticket to Integration team   ###  Related links  * [Container process ](https://app.datadoghq.com/serverless/azure/container-apps?fromUser=false&group=&panel_end=1757367600000&panel_paused=false&panel_start=1757364000000&selection=container_apps%2Bc9decce4-d5a4-4daf-b0b9-a3c82aa75f3b%2Besb-connector-service%2Barg-aue-npdsi-eiq-be-01&text_search=esb-connector&start=1757364000000&end=1757367600000&paused=false) * [System Dashboards](https://app.datadoghq.com/dashboard/lists)  @slack-eiq-infrastructure-alerts @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/a7f0ac808f336283b592c2b5b9834ad756729259.png)](/monitors/212107528?group=http.status_code%3A500&from_ts=1758660508000&to_ts=1758661708000&event_id=8294632145229135372&link_source=monitor_notif)  **trace.express.request.errors** over **env:prod,http.status_code:500,service:esb-connector-service** was **> 50.0** in total during the **last 5m**.  The monitor was last triggered at Tue Sep 23 2025 21:03:28 UTC.  - - -  [[Monitor Status](/monitors/212107528?group=http.status_code%3A500&from_ts=1758660508000&to_ts=1758661708000&event_id=8294632145229135372&link_source=monitor_notif)] · [[Edit Monitor](/monitors/212107528/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28service%3Aesb-connector-service%2C+env%3Aprod%29+AND+http.status_code%3A500&from_ts=1758661108000&to_ts=1758661408000&live=false&link_source=monitor_notif)] %%%",2025-09-23 21:03:28,"env:prod,http.status_code:500,monitor,priority:p3,service:esb-connector-service,source:alert,team:ge-digital-services,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-loyaltyaccounts/_/vouchers.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/e40feee587463220bdd2d304ed2a12894d476d38.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fvouchers&from_ts=1758659879000&to_ts=1758661079000&event_id=8294622704178501858&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:52:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fvouchers&from_ts=1758659879000&to_ts=1758661079000&event_id=8294622704178501858&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fvouchers&from_ts=1758646379000&to_ts=1758660779000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:52:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/vouchers,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-loyaltyaccounts/_/bookings.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/aa6e6d8eaccd9d6787ba8882caff10b76f041c47.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbookings&from_ts=1758659879000&to_ts=1758661079000&event_id=8294622704024252736&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:52:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbookings&from_ts=1758659879000&to_ts=1758661079000&event_id=8294622704024252736&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbookings&from_ts=1758646379000&to_ts=1758660779000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:52:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/bookings,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/esb/finance/loyaltyvouchers/v1/expiring-vouchers}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/esb/finance/loyaltyvouchers/v1/expiring-vouchers.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/f85333b02ee25cbad439172488337aafcd6fc1bf.png)](/monitors/213661559?group=resource_name%3Aget_%2Fesb%2Ffinance%2Floyaltyvouchers%2Fv1%2Fexpiring-vouchers&from_ts=1758659759000&to_ts=1758660959000&event_id=8294620654466912317&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:50:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fesb%2Ffinance%2Floyaltyvouchers%2Fv1%2Fexpiring-vouchers&from_ts=1758659759000&to_ts=1758660959000&event_id=8294620654466912317&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fesb%2Ffinance%2Floyaltyvouchers%2Fv1%2Fexpiring-vouchers&from_ts=1758646259000&to_ts=1758660659000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:50:59,"env:prod,monitor,priority:p3,resource_name:get_/esb/finance/loyaltyvouchers/v1/expiring-vouchers,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/financial/getlastbill/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/financial/getlastbill/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/d1bd7f6c55d8eaba2aa9af6ed9666bf71bdb0def.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758659759000&to_ts=1758660959000&event_id=8294620654874717267&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:50:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758659759000&to_ts=1758660959000&event_id=8294620654874717267&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Ffinancial%2Fgetlastbill%2Fv1&from_ts=1758646259000&to_ts=1758660659000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:50:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/financial/getlastbill/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/4f47369e7153092fa113ba0e2e3ab0bf750e8897.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbalance%2Fexpiring-vouchers&from_ts=1758659759000&to_ts=1758660959000&event_id=8294620655017049385&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:50:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbalance%2Fexpiring-vouchers&from_ts=1758659759000&to_ts=1758660959000&event_id=8294620655017049385&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fcustomers-loyaltyaccounts%2F_%2Fbalance%2Fexpiring-vouchers&from_ts=1758646259000&to_ts=1758660659000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:50:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/customers-loyaltyaccounts/_/balance/expiring-vouchers,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/productandcontract/getsupplyagreement/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/productandcontract/getsupplyagreement/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/bdf345dadb3dc6a10fb04e94fffeaaf400e91bc0.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fgetsupplyagreement%2Fv1&from_ts=1758659699000&to_ts=1758660899000&event_id=8294619654079507987&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:49:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fgetsupplyagreement%2Fv1&from_ts=1758659699000&to_ts=1758660899000&event_id=8294619654079507987&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fproductandcontract%2Fgetsupplyagreement%2Fv1&from_ts=1758646199000&to_ts=1758660599000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:49:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/productandcontract/getsupplyagreement/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/get}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/get.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/479d99f9e6dac12a1466e3eb331e4731ddec0f59.png)](/monitors/213661559?group=resource_name%3Apost_%2Fget&from_ts=1758658919000&to_ts=1758660119000&event_id=8294606593947521232&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:36:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fget&from_ts=1758658919000&to_ts=1758660119000&event_id=8294606593947521232&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fget&from_ts=1758645419000&to_ts=1758659819000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:36:59,"env:prod,monitor,priority:p3,resource_name:post_/get,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request get_/v1/exp/productandcontract/supplyagreements/_/pricing.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/47ee6476229d6b6b1ed9a4dc8f67f79382367126.png)](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758658739000&to_ts=1758659939000&event_id=8294603572476323675&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:33:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758658739000&to_ts=1758659939000&event_id=8294603572476323675&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Aget_%2Fv1%2Fexp%2Fproductandcontract%2Fsupplyagreements%2F_%2Fpricing&from_ts=1758645239000&to_ts=1758659639000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:33:59,"env:prod,monitor,priority:p3,resource_name:get_/v1/exp/productandcontract/supplyagreements/_/pricing,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P3] [Triggered on {resource_name:post_/esb/pricing/getsupplyagreementpricing/v1}] [P3] [Tier 1] Mule Increased Latency detected for Mule Requests,"%%% Increased Latency detected for Mule Request post_/esb/pricing/getsupplyagreementpricing/v1.  Increased Latency detected for the last 15 minutes . This latency is abnormal for this time.  ### Related links  -   [API Catalog (Endpoints) ](https://app.datadoghq.com/software?env=prod&fromUser=false&selectedComponent=endpoint&start=1757989687039&end=1757993287039) -   [Integration Dashboard ](https://app.datadoghq.com/dashboard/raf-4rd-tca/integration-production?fromUser=false&refresh_mode=sliding&tpl_var_host%5B0%5D=%2A&from_ts=1757989812916&to_ts=1757993412916&live=true) -   [Live Processes](https://app.datadoghq.com/process)    @slack-integration-support @teams-monitoring @webhook-Rootly  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/e7a09f19626421af8110186c6c5145aea9d91e9f.png)](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758658739000&to_ts=1758659939000&event_id=8294603572499695566&link_source=monitor_notif)  At least **100%** of **`sum:trace.grizzly.request{env:prod,span.kind:server}.as_count()`** values have been more than **3** deviations **above** the predicted values during the **last 15m**.  The monitor was last triggered at Tue Sep 23 2025 20:33:59 UTC.  - - -  [[Monitor Status](/monitors/213661559?group=resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758658739000&to_ts=1758659939000&event_id=8294603572499695566&link_source=monitor_notif)] · [[Edit Monitor](/monitors/213661559/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28env%3Aprod%2Cspan.kind%3Aserver%29+AND+resource_name%3Apost_%2Fesb%2Fpricing%2Fgetsupplyagreementpricing%2Fv1&from_ts=1758645239000&to_ts=1758659639000&live=false&link_source=monitor_notif)] %%%",2025-09-23 20:33:59,"env:prod,monitor,priority:p3,resource_name:post_/esb/pricing/getsupplyagreementpricing/v1,service:mule,source:alert,span.kind:server,team:ge-integration,tier:tier_1",error
[P5] [Recovered] [Tier 3] UAT NGINX error rate has increased,"%%%  UAT NGINX is reporting an elevated error rate of `0.75%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.        ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/27a1aef713c34521db1f43bbd5609d5a1ee94bda.png)](/monitors/204747377?from_ts=1758647897000&to_ts=1758649097000&event_id=8294421584550200880&link_source=monitor_notif)  At least **75%** of **`(sum:universal.http.server{env:uat,error:true,service:mule}.as_count() / sum:universal.http.server{env:uat,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Tue Sep 23 2025 16:31:17 UTC.  - - -  [[Monitor Status](/monitors/204747377?from_ts=1758647897000&to_ts=1758649097000&event_id=8294421584550200880&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204747377/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2C+env%3Auat%29+OR+%28service%3Amule+%2Cenv%3Auat%29&from_ts=1758634397000&to_ts=1758648797000&live=false&link_source=monitor_notif)] %%%",2025-09-23 17:33:17,"env:uat,error:true,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",success
[P5] [Triggered] [Tier 3] UAT NGINX error rate has increased,"%%%  UAT NGINX error rate is at  0.75% over the last 5 minutes. UAT NGINX is reporting an elevated error rate of `0.75%` for Mule traffic over the past 5 minutes. This may indicate issues with upstream services, misconfigurations, or traffic anomalies.  Impact  -   **Service Reliability:** Increased 4xx/5xx errors may result in failed or degraded API responses. -   **User Experience:** End users may encounter timeouts, gateway errors, or failed transactions. -   **Downstream Systems:** Potential cascading failures or retries in dependent service  Troubleshooting steps  1.  **Check NGINX Metrics :**\     Look for spikes in 4xx/5xx errors and identify affected endpoints. 3.  **Review Recent Deployments:**\     Check for recent changes to Mule APIs, NGINX config, or upstream services. 4.  **Validate Upstream Health:**\     Ensure Mule services are healthy and responsive. Mule API Gateway hosts (*rt-mule-p01) and container, esb-connector-service  5.  **Check Traffic Patterns:**\     Look for unusual spikes or patterns in request volume.        ***  ### Related links    [Host Map](https://app.datadoghq.com/infrastructure/map)  [Live Processes](https://app.datadoghq.com/process)   [System Dashboards](https://app.datadoghq.com/dashboard/lists)   [Monitor Documentation](https://docs.datadoghq.com/monitors/)  ***  [![Metric Graph](https://p.datadoghq.com/snapshot/view/dd-snapshots-prod/org_1330387/2025-09-23/5f0c341bfb15f7c4331f9858fa3ce36bd3130c6e.png)](/monitors/204747377?from_ts=1758644177000&to_ts=1758645377000&event_id=8294359177636190235&link_source=monitor_notif)  At least **75%** of **`(sum:universal.http.server{env:uat,error:true,service:mule}.as_count() / sum:universal.http.server{env:uat,service:mule}.as_count())`** values have been more than **2** deviations from the predicted values during the **last 10m**.  The monitor was last triggered at Tue Sep 23 2025 16:31:17 UTC.  - - -  [[Monitor Status](/monitors/204747377?from_ts=1758644177000&to_ts=1758645377000&event_id=8294359177636190235&link_source=monitor_notif)] · [[Edit Monitor](/monitors/204747377/edit?link_source=monitor_notif)] · [[Related Logs](/logs?query=%28error%3Atrue+%2Cservice%3Amule+%2C+env%3Auat%29+OR+%28service%3Amule+%2Cenv%3Auat%29&from_ts=1758630677000&to_ts=1758645077000&live=false&link_source=monitor_notif)] %%%",2025-09-23 16:31:17,"env:uat,error:true,monitor,priority:p5,service:mule,source:alert,team:ge-integration,tier:tier_3",error
